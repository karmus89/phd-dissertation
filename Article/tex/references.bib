@article{Bose2016,
  author   = {Bose, Pritam and Kasabov, Nikola K. and Bruzzone, Lorenzo and Hartono, Reggio N.},
  doi      = {10.1109/TGRS.2016.2586602},
  issn     = {01962892},
  journal  = {IEEE Transactions on Geoscience and Remote Sensing},
  keywords = {Crop yield forecasting,Moderate Resolution Imaging Spectroradiometer (MOD,Spiking neural networks (SNNs),estimation,machine learning,normalized difference vegetation index (NDVI),remote sensing},
  number   = {11},
  pages    = {6563--6573},
  title    = {{Spiking Neural Networks for Crop Yield Estimation Based on Spatiotemporal Analysis of Image Time Series}},
  volume   = {54},
  year     = {2016}
}

@book{Goodfellow-et-al-2016,
  annote    = {$\backslash$url{\{}http://www.deeplearningbook.org{\}}},
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  publisher = {MIT Press},
  title     = {{Deep Learning}},
  year      = {2016}
}

@article{Kamilaris2017,
  author    = {Kamilaris, Andreas and Kartakoullis, Andreas and Prenafeta-Bold{\'{u}}, Francesc X.},
  doi       = {10.1016/J.COMPAG.2017.09.037},
  issn      = {0168-1699},
  journal   = {Computers and Electronics in Agriculture},
  pages     = {23--37},
  publisher = {Elsevier},
  title     = {{A review on the practice of big data analysis in agriculture}},
  volume    = {143},
  year      = {2017}
}

@article{Luke2017,
  author      = {Laine, Antti and H{\"{o}}gn{\"{a}}sbacka, Merja and Niskanen, Markku and Ohralahti, Kalle and Jauhiainen, Lauri and Kaseva, Janne and Nikander, Hannele},
  institution = {Luonnonvarakeskus},
  pages       = {262},
  title       = {{Virallisten lajikekokeiden tulokset 2009-2016}},
  year        = {2017}
}

@article{Nevavuori2019,
  author    = {Nevavuori, Petteri and Narra, Nathaniel and Lipping, Tarmo},
  doi       = {10.1016/j.compag.2019.104859},
  issn      = {01681699},
  journal   = {Computers and Electronics in Agriculture},
  keywords  = {Barley,Convolutional neural network,Crop yield prediction,Growth phase,Multispectral,NDVI,UAV,Wheat,convolutional neural network,crop yield prediction},
  number    = {June},
  pages     = {104859},
  publisher = {Elsevier},
  title     = {{Crop yield prediction with deep convolutional neural networks}},
  volume    = {163},
  year      = {2019}
}

@online{ESAS2,
  note    = {\url{https://sentinel.esa.int/web/sentinel/missions/sentinel-2}},
  title   = {{ESA: Sentinel-2}},
  urldate = {2018-04-12}
}

@online{Copernicus,
  author  = {ESA},
  note    = {\url{https://scihub.copernicus.eu/}},
  title   = {{Open Access Hub}},
  urldate = {2019-12-17}
}

@online{ESAS2L2,
  author  = {ESA},
  note    = {\url{https://earth.esa.int/web/sentinel/technical-guides/sentinel-2-msi/level-2a/algorithm}},
  title   = {{Level-2A Algorithm - Sentinel-2 MSI Technical Guide - Sentinel Online}},
  urldate = {2019-12-17}
}

@online{Paituli,
  note    = {\url{https://avaa.tdata.fi/web/paituli/latauspalvelu}},
  title   = {{PaITuli - Spatial data for research and teaching}},
  urldate = {2020-11-09}
}

@online{NationalLandSurveyofFinland,
  author  = {{National Land Survey of Finland}},
  note    = {\url{https://www.maanmittauslaitos.fi/en/maps-and-spatial-data/expert-users/product-descriptions/elevation-model-2-m}},
  title   = {{Elevation model 2 m}},
  urldate = {2020-11-09}
}

@online{Warmerdam1998,
  author  = {Warmerdam, Frank and Rouault, Even},
  note    = {\url{https://gdal.org/ gdal.org/index.html}},
  title   = {{GDAL — GDAL documentation}},
  urldate = {2020-12-07},
  year    = {1998}
}

@article{Nevavuori2020,
  abstract  = {Unmanned aerial vehicle (UAV) based remote sensing is gaining momentum worldwide in a variety of agricultural and environmental monitoring and modelling applications. At the same time, the increasing availability of yield monitoring devices in harvesters enables input-target mapping of in-season RGB and crop yield data in a resolution otherwise unattainable by openly availabe satellite sensor systems. Using time series UAV RGB and weather data collected from nine crop fields in Pori, Finland, we evaluated the feasibility of spatio-temporal deep learning architectures in crop yield time series modelling and prediction with RGB time series data. Using Convolutional Neural Networks (CNN) and Long-Short Term Memory (LSTM) networks as spatial and temporal base architectures, we developed and trained CNN-LSTM, convolutional LSTM and 3D-CNN architectures with full 15 week image frame sequences from the whole growing season of 2018. The best performing architecture, the 3D-CNN, was then evaluated with several shorter frame sequence configurations from the beginning of the season. With 3D-CNN, we were able to achieve 218.9 kg/ha mean absolute error (MAE) and 5.51{\%} mean absolute percentage error (MAPE) performance with full length sequences. The best shorter length sequence performance with the same model was 292.8 kg/ha MAE and 7.17{\%} MAPE with four weekly frames from the beginning of the season.},
  author    = {Nevavuori, Petteri and Narra, Nathaniel and Linna, Petri and Lipping, Tarmo},
  doi       = {10.3390/rs12234000},
  issn      = {2072-4292},
  journal   = {Remote Sensing},
  keywords  = {3d,UAV,cnn,convolutional lstm,crop yield prediction,deep learning,lstm,spatio,temporal modelling,time series},
  number    = {23},
  pages     = {4000},
  publisher = {Multidisciplinary Digital Publishing Institute},
  title     = {{Crop Yield Prediction Using Multitemporal UAV Data and Spatio-Temporal Deep Learning Models}},
  volume    = {12},
  year      = {2020}
}

@article{VanKlompenburg2020,
  abstract  = {Machine learning is an important decision support tool for crop yield prediction, including supporting decisions on what crops to grow and what to do during the growing season of the crops. Several machine learning algorithms have been applied to support crop yield prediction research. In this study, we performed a Systematic Literature Review (SLR) to extract and synthesize the algorithms and features that have been used in crop yield prediction studies. Based on our search criteria, we retrieved 567 relevant studies from six electronic databases, of which we have selected 50 studies for further analysis using inclusion and exclusion criteria. We investigated these selected studies carefully, analyzed the methods and features used, and provided suggestions for further research. According to our analysis, the most used features are temperature, rainfall, and soil type, and the most applied algorithm is Artificial Neural Networks in these models. After this observation based on the analysis of machine learning-based 50 papers, we performed an additional search in electronic databases to identify deep learning-based studies, reached 30 deep learning-based papers, and extracted the applied deep learning algorithms. According to this additional analysis, Convolutional Neural Networks (CNN) is the most widely used deep learning algorithm in these studies, and the other widely used deep learning algorithms are Long-Short Term Memory (LSTM) and Deep Neural Networks (DNN).},
  author    = {van Klompenburg, Thomas and Kassahun, Ayalew and Catal, Cagatay},
  doi       = {10.1016/j.compag.2020.105709},
  issn      = {01681699},
  journal   = {Computers and Electronics in Agriculture},
  keywords  = {Crop yield prediction,Decision support system,Deep learning,Machine learning,Systematic literature review},
  pages     = {105709},
  publisher = {Elsevier B.V.},
  title     = {{Crop yield prediction using machine learning: A systematic literature review}},
  volume    = {177},
  year      = {2020}
}

@article{Yang2019,
  abstract  = {Forecasting rice grain yield prior to harvest is essential for crop management, food security evaluation, food trade, and policy-making. Many successful applications have been made in crop yield estimation using remotely sensed products, such as vegetation index (VI) from multispectral imagery. However, VI-based approaches are only suitable for estimating rice grain yield at the middle stage of growth but have limited capability at the ripening stage. In this study, an efficient convolutional neural network (CNN) architecture was proposed to learn the important features related to rice grain yield from low-altitude remotely sensed imagery. In one major region for rice cultivation of Southern China, a 160-hectare site with over 800 management units was chosen to investigate the ability of CNN in rice grain yield estimation. The datasets of RGB and multispectral images were obtained by a fixed-wing, unmanned aerial vehicle (UAV), which was mounted with a digital camera and multispectral sensors. The network was trained with different datasets and compared against the traditional vegetation index-based method. In addition, the temporal and spatial generality of the trained network was investigated. The results showed that the CNNs trained by RGB and multispectral datasets perform much better than VIs-based regression model for rice grain yield estimation at the ripening stage. The RGB imagery of very high spatial resolution contains important spatial features with respect to grain yield distribution, which can be learned by deep CNN. The results highlight the promising potential of deep convolutional neural networks for rice grain yield estimation with excellent spatial and temporal generality, and a wider time window of yield forecasting.},
  author    = {Yang, Qi and Shi, Liangsheng and Han, Jinye and Zha, Yuanyuan and Zhu, Penghui},
  doi       = {10.1016/j.fcr.2019.02.022},
  issn      = {03784290},
  journal   = {Field Crops Research},
  keywords  = {CNN,Deep learning,Rice crop,UAV,Yield estimation},
  number    = {August 2018},
  pages     = {142--153},
  publisher = {Elsevier},
  title     = {{Deep convolutional neural networks for rice grain yield estimation at the ripening stage using UAV-based remotely sensed images}},
  volume    = {235},
  year      = {2019}
}

@article{Tedesco-Oliveira2020,
  abstract  = {One way to improve the quality of mechanized cotton harvesting is to change harvester settings and adjustments throughout the process, according to information obtained during the operation. We believe that yield predictions are important for managing the quality of operation, aiming at increasing efficiency and reducing losses. Therefore, this study aimed to develop an automated system for cotton yield prediction from color images acquired by a simple mobile device. We propose a robust approach to environmental conditions, training detection algorithms with images acquired at different times throughout the day, and evaluating three different scenarios (low-, average-, and high-demand computational resources). The experimental results for the average demand computational scenario, which are suitable for real-time deployment on low-cost devices such as smartphones and other ARM-processed devices, indicated the possibility of counting bolls using images acquired at different times throughout the day, with mean errors of 8.84{\%} (∼5 bolls). Furthermore, we observed a 17.86{\%} error when predicting yield using 205 images from the testing dataset, which is equivalent to about 19.14 g.},
  author    = {Tedesco-Oliveira, Danilo and {Pereira da Silva}, Rouverson and Maldonado, Walter and Zerbato, Cristiano},
  doi       = {10.1016/j.compag.2020.105307},
  issn      = {01681699},
  journal   = {Computers and Electronics in Agriculture},
  keywords  = {Deep learning,Object detection,Smart harvesting,Yield prediction},
  pages     = {105307},
  publisher = {Elsevier B.V.},
  title     = {{Convolutional neural networks in predicting cotton yield from images of commercial fields}},
  volume    = {171},
  year      = {2020}
}

@article{Pathan2018,
  archiveprefix = {arXiv},
  arxivid       = {1808.07553},
  author        = {Pathan, Sharmin and Hong, Yi},
  eprint        = {1808.07553},
  title         = {{Predictive Image Regression for Longitudinal Studies with Missing Data}},
  year          = {2018}
}

@article{Yao2017,
  archiveprefix = {arXiv},
  arxivid       = {1710.09757},
  author        = {Yao, Haiyan and Han, Kang and Wan, Wanggen and Hou, Li},
  eprint        = {1710.09757},
  keywords      = {convolutional neural network,crowd counting,long short term memory (LSTM),spatial regression},
  title         = {{Deep Spatial Regression Model for Image Crowd Counting}},
  year          = {2017}
}

@inproceedings{Sainath2015,
  author    = {Sainath, Tara N. and Vinyals, Oriol and Senior, Andrew and Sak, Hasim},
  booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  issn      = {15206149},
  pages     = {4580--4584},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  title     = {{Convolutional, Long Short-Term Memory, fully connected Deep Neural Networks}},
  volume    = {2015-Augus},
  year      = {2015}
}

@article{He2015,
  archiveprefix = {arXiv},
  arxivid       = {1502.01852},
  author        = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  eprint        = {1502.01852},
  issn          = {15505499},
  journal       = {Proceedings of the IEEE International Conference on Computer Vision},
  pages         = {1026--1034},
  pmid          = {7410480},
  title         = {{Delving deep into rectifiers: Surpassing human-level performance on imagenet classification}},
  volume        = {2015 Inter},
  year          = {2015}
}

@inproceedings{Grinciunaite2016,
  archiveprefix = {arXiv},
  arxivid       = {1609.00036},
  author        = {Grinciunaite, Agne and Gudi, Amogh and Tasli, Emrah and {Den Uyl}, Marten},
  booktitle     = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  eprint        = {1609.00036},
  issn          = {16113349},
  pages         = {32--39},
  title         = {{Human pose estimation in space and time using 3D CNN}},
  volume        = {9915 LNCS},
  year          = {2016}
}

@article{Tran2015,
  archiveprefix = {arXiv},
  arxivid       = {1412.0767},
  author        = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
  eprint        = {1412.0767},
  issn          = {15505499},
  journal       = {Proceedings of the IEEE International Conference on Computer Vision},
  pages         = {4489--4497},
  pmid          = {22392705},
  title         = {{Learning spatiotemporal features with 3D convolutional networks}},
  volume        = {2015 Inter},
  year          = {2015}
}

@article{Kim2019,
  abstract  = {Forecasting stock prices plays an important role in setting a trading strategy or determining the appropriate timing for buying or selling a stock. We propose a model, called the feature fusion long short-term memory-convolutional neural network (LSTM-CNN) model, that combines features learned from different representations of the same data, namely, stock time series and stock chart images, to predict stock prices. The proposed model is composed of LSTM and a CNN, which are utilized for extracting temporal features and image features. We measure the performance of the proposed model relative to those of single models (CNN and LSTM) using SPDR S{\&}P 500 ETF data. Our feature fusion LSTM-CNN model outperforms the single models in predicting stock prices. In addition, we discover that a candlestick chart is the most appropriate stock chart image to use to forecast stock prices. Thus, this study shows that prediction error can be efficiently reduced by using a combination of temporal and image features from the same data rather than using these features separately.},
  author    = {Kim, Taewook and Kim, Ha Young},
  doi       = {10.1371/journal.pone.0212320},
  editor    = {{Hernandez Montoya}, Alejandro Raul},
  issn      = {19326203},
  journal   = {PLoS ONE},
  keywords  = {Artificial neural networks,Charts,Data visualization,Finance,Forecasting,Imaging techniques,Machine learning,Recurrent neural networks},
  number    = {2},
  pages     = {e0212320},
  pmid      = {30768647},
  publisher = {Public Library of Science},
  title     = {{Forecasting stock prices with a feature fusion LSTM-CNN model using different representations of the same data}},
  volume    = {14},
  year      = {2019}
}

@article{Liu2018,
  abstract  = {At present, realizing high-quality automatic welding through online monitoring is a research focus in engineering applications. In this paper, a CNN-LSTM algorithm is proposed, which combines the advantages of convolutional neural networks (CNNs) and long short-term memory networks (LSTMs). The CNN-LSTM algorithm establishes a shallow CNN to extract the primary features of the molten pool image. Then the feature tensor extracted by the CNN is transformed into the feature matrix. Finally, the rows of the feature matrix are fed into the LSTM network for feature fusion. This process realizes the implicit mapping from molten pool images to welding defects. The test results on the self-made molten pool image dataset show that CNN contributes to the overall feasibility of the CNN-LSTM algorithm and LSTM network is the most superior in the feature hybrid stage. The algorithm converges at 300 epochs and the accuracy of defects detection in CO2 welding molten pool is 94{\%}. The processing time of a single image is 0.067 ms, which fully meets the real-time monitoring requirement based on molten pool image. The experimental results on the MNIST and FashionMNIST datasets show that the algorithm is universal and can be used for similar image recognition and classification tasks.},
  author    = {Liu, Tianyuan and Bao, Jinsong and Wang, Junliang and Zhang, Yiming},
  doi       = {10.3390/S18124369},
  issn      = {14248220},
  journal   = {Sensors (Switzerland)},
  keywords  = {CNN,CO2 welding,Deep learning,LSTM,Molten pool,Online monitoring},
  number    = {12},
  pages     = {4369},
  publisher = {MDPI AG},
  title     = {{A hybrid CNN-LSTM algorithm for online defect recognition of CO2 welding}},
  volume    = {18},
  year      = {2018}
}

@article{Huang2018a,
  abstract = {In modern society, air pollution is an important topic as this pollution exerts a critically bad influence on human health and the environment. Among air pollutants, Particulate Matter (PM 2.5) consists of suspended particles with a diameter equal to or less than 2.5 µm. Sources of PM 2.5 can be coal-fired power generation, smoke, or dusts. These suspended particles in the air can damage the respiratory and cardiovascular systems of the human body, which may further lead to other diseases such as asthma, lung cancer, or cardiovascular diseases. To monitor and estimate the PM 2.5 concentration, Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) are combined and applied to the PM 2.5 forecasting system. To compare the overall performance of each algorithm, four measurement indexes, Mean Absolute Error (MAE), Root Mean Square Error (RMSE) Pearson correlation coefficient and Index of Agreement (IA) are applied to the experiments in this paper. Compared with other machine learning methods, the experimental results showed that the forecasting accuracy of the proposed CNN-LSTM model (APNet) is verified to be the highest in this paper. For the CNN-LSTM model, its feasibility and practicability to forecast the PM 2.5 concentration are also verified in this paper. The main contribution of this paper is to develop a deep neural network model that integrates the CNN and LSTM architectures, and through historical data such as cumulated hours of rain, cumulated wind speed and PM 2.5 concentration. In the future, this study can also be applied to the prevention and control of PM 2.5 .},
  author   = {Huang, Chiou-Jye and Kuo, Ping-Huan},
  doi      = {10.3390/s18072220},
  keywords = {CNN-LSTM model,PM2.5 forecasting,big data analytics,deep learning},
  title    = {{A Deep CNN-LSTM Model for Particulate Matter (PM 2.5 ) Forecasting in Smart Cities}},
  year     = {2018}
}

@article{Tsironi2017,
  abstract  = {In this research, we analyze a Convolutional Long Short-Term Memory Recurrent Neural Network (CNNLSTM) in the context of gesture recognition. CNNLSTMs are able to successfully learn gestures of varying duration and complexity. For this reason, we analyze the architecture by presenting a qualitative evaluation of the model, based on the visualization of the internal representations of the convolutional layers and on the examination of the temporal classification outputs at a frame level, in order to check if they match the cognitive perception of a gesture. We show that CNNLSTM learns the temporal evolution of the gestures classifying correctly their meaningful part, known as Kendon's stroke phase. With the visualization, for which we use the deconvolution process that maps specific feature map activations to original image pixels, we show that the network learns to detect the most intense body motion. Finally, we show that CNNLSTM outperforms both plain CNN and LSTM in gesture recognition.},
  author    = {Tsironi, Eleni and Barros, Pablo and Weber, Cornelius and Wermter, Stefan},
  doi       = {10.1016/j.neucom.2016.12.088},
  issn      = {18728286},
  journal   = {Neurocomputing},
  keywords  = {CNN,CNN visualization,Gesture recognition,LSTM},
  pages     = {76--86},
  publisher = {Elsevier B.V.},
  title     = {{An analysis of Convolutional Long Short-Term Memory Recurrent Neural Networks for gesture recognition}},
  volume    = {268},
  year      = {2017}
}

@article{Zapata-Impata2019,
  abstract  = {{\textless}p{\textgreater}Robotic manipulators have to constantly deal with the complex task of detecting whether a grasp is stable or, in contrast, whether the grasped object is slipping. Recognising the type of slippage—translational, rotational—and its direction is more challenging than detecting only stability, but is simultaneously of greater use as regards correcting the aforementioned grasping issues. In this work, we propose a learning methodology for detecting the direction of a slip (seven categories) using spatio-temporal tactile features learnt from one tactile sensor. Tactile readings are, therefore, pre-processed and fed to a ConvLSTM that learns to detect these directions with just 50 ms of data. We have extensively evaluated the performance of the system and have achieved relatively high results at the detection of the direction of slip on unseen objects with familiar properties (82.56{\%} accuracy).{\textless}/p{\textgreater}},
  author    = {Zapata-Impata, Brayan and Gil, Pablo and Torres, Fernando},
  doi       = {10.3390/s19030523},
  issn      = {1424-8220},
  journal   = {Sensors},
  keywords  = {Deep learning,Direction of slip,Spatio-temporal feature learning,Tactile processing},
  number    = {3},
  pages     = {523},
  publisher = {MDPI AG},
  title     = {{Learning Spatio Temporal Tactile Features with a ConvLSTM for the Direction Of Slip Detection}},
  volume    = {19},
  year      = {2019}
}

@article{Shi2015a,
  abstract      = {The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.},
  archiveprefix = {arXiv},
  arxivid       = {1506.04214},
  author        = {Shi, Xingjian and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-kin and Woo, Wang-chun},
  eprint        = {1506.04214},
  issn          = {10495258},
  title         = {{Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting}},
  year          = {2015}
}

@article{Yue2020,
  abstract  = {An accurate forecast of daily meteorological factors throughout the year is not only of great significance for the study of climate trends in a certain area but also enables the prediction of crop growth stages. Moreover, the prediction of crop growth stages is related to the scheduling of planting and tillage, the determination of machine harvest time, and the prediction of crop yield. However, highly complex dynamics cause large volatility in meteorological factors, so it is very challenging to predict the crop growth stage accurately, based on weather data. To solve this problem, we propose a data-driven encoder-decoder model, using long short-term memory (LSTM) and convolutional LSTM (ConvLSTM), which can be applied to forecast daily sunshine duration, cumulative precipitation, and average temperature for the coming year. To further test the performance of the ConvLSTM-based model, it is compared with the conventional LSTM encoder-decoder model and the convolutional neural network (CNN)-LSTM encoder-decoder model. The results demonstrate that, the ConvLSTM-based model is more accurate than the others for forecasting temperature (MAE = 2.602 °C, RMSE = 3.456 °C), precipitation (MAE = 3.878 mm, RMSE = 10.503 mm), and sunshine hours (MAE = 3.445 h, RMSE = 4.172 h) in 2014–2016. Furthermore, precise forecasting of meteorological factors allows us to develop a hybrid model and a data-driven model for the prediction of each growth stage separately. The hybrid model combines the ConvLSTM encoder-decoder model with empirical models, whereas the data-driven model comprises the ConvLSTM encoder-decoder model and traditional neural network structures. Finally, we compared the two types of models on a real-world dataset from Dandong, and concluded that the data-driven model is more accurate than the hybrid model for prediction of maize growth stages, with R2 in the range of 0.755–0.883, MAE 0.588–2.205 days, and RMSE 0.978–2.729 days. In the future, these models can also be used to predict the growth stages of other crops.},
  author    = {Yue, Yang and Li, Jin Hai and Fan, Li Feng and Zhang, Li Li and Zhao, Peng Fei and Zhou, Qiao and Wang, Nan and Wang, Zhong Yi and Huang, Lan and Dong, Xue Hui},
  doi       = {10.1016/j.compag.2020.105351},
  issn      = {01681699},
  journal   = {Computers and Electronics in Agriculture},
  keywords  = {ConvLSTM encoder-decoder model,Daily weather forecast,Data-driven model,Hybrid model,Maize growth stages},
  pages     = {105351},
  publisher = {Elsevier B.V.},
  title     = {{Prediction of maize growth stages based on deep learning}},
  volume    = {172},
  year      = {2020}
}

@article{Yaramasu2020,
  author    = {Yaramasu, Raghu and Bandaru, Varaprasad and Pnvr, Koutilya},
  doi       = {10.1016/j.compag.2020.105664},
  issn      = {01681699},
  journal   = {Computers and Electronics in Agriculture},
  pages     = {105664},
  publisher = {Elsevier},
  title     = {{Pre-season crop type mapping using deep neural networks}},
  volume    = {176},
  year      = {2020}
}

@inproceedings{Rustowicz2019,
  abstract  = {Automatic, accurate crop type maps can provide unprecedented information for understanding food systems, especially in developing countries where ground surveys are infrequent. However, little work has applied existing methods to these data scarce environments, which also have unique challenges of irregularly shaped fields, frequent cloud coverage, small plots, and a severe lack of training data. To address this gap in the literature, we provide the first crop type semantic segmentation dataset of small holder farms, specifically in Ghana and South Sudan. We are also the first to utilize high resolution, high frequency satellite data in segmenting small holder farms. Despite the challenges, we achieve an average F1 score and overall accuracy of 57.3 and 60.9{\%} in Ghana and 69.7 and 85.3{\%} in South Sudan. Additionally , our approach outperforms the state-of-the-art method in a data-rich setting of Germany by over 8 points in F1 and 6 points in accuracy. Code and a link to the dataset are publicly available at github. com/roserustowicz/crop-type-mapping.},
  author    = {Rustowicz, Rose and Cheong, Robin and Wang, Lijing and Ermon, Stefano and Burke, Marshall and Lobell, David},
  booktitle = {CVPR Workshops},
  pages     = {75--82},
  title     = {{Semantic Segmentation of Crop Type in Africa: A Novel Dataset and Analysis of Deep Learning Methods}},
  year      = {2019}
}

@article{Sun2019,
  abstract  = {{\textless}p{\textgreater}Yield prediction is of great significance for yield mapping, crop market planning, crop insurance, and harvest management. Remote sensing is becoming increasingly important in crop yield prediction. Based on remote sensing data, great progress has been made in this field by using machine learning, especially the Deep Learning (DL) method, including Convolutional Neural Network (CNN) or Long Short-Term Memory (LSTM). Recent experiments in this area suggested that CNN can explore more spatial features and LSTM has the ability to reveal phenological characteristics, which both play an important role in crop yield prediction. However, very few experiments combining these two models for crop yield prediction have been reported. In this paper, we propose a deep CNN-LSTM model for both end-of-season and in-season soybean yield prediction in CONUS at the county-level. The model was trained by crop growth variables and environment variables, which include weather data, MODIS Land Surface Temperature (LST) data, and MODIS Surface Reflectance (SR) data; historical soybean yield data were employed as labels. Based on the Google Earth Engine (GEE), all these training data were combined and transformed into histogram-based tensors for deep learning. The results of the experiment indicate that the prediction performance of the proposed CNN-LSTM model can outperform the pure CNN or LSTM model in both end-of-season and in-season. The proposed method shows great potential in improving the accuracy of yield prediction for other crops like corn, wheat, and potatoes at fine scales in the future.{\textless}/p{\textgreater}},
  author    = {Sun, Jie and Di, Liping and Sun, Ziheng and Shen, Yonglin and Lai, Zulong},
  doi       = {10.3390/s19204363},
  issn      = {1424-8220},
  journal   = {Sensors},
  keywords  = {CNN-LSTM,County-level,Google Earth Engine,Soybean,Yield prediction},
  number    = {20},
  pages     = {4363},
  publisher = {MDPI AG},
  title     = {{County-Level Soybean Yield Prediction Using Deep CNN-LSTM Model}},
  volume    = {19},
  year      = {2019}
}

@inproceedings{Simonyan2015,
  abstract      = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  arxivid       = {1409.1556v6},
  author        = {Simonyan, Karen and Zisserman, Andrew},
  booktitle     = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
  eprint        = {1409.1556v6},
  keywords      = {()},
  title         = {{Very deep convolutional networks for large-scale image recognition}},
  year          = {2015}
}

@inproceedings{Huang2017,
  abstract      = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L2+1) direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at github.com/liuzhuang13/DenseNet.},
  archiveprefix = {arXiv},
  arxivid       = {1608.06993},
  author        = {Huang, Gao and Liu, Zhuang and {Van Der Maaten}, Laurens and Weinberger, Kilian Q},
  booktitle     = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
  doi           = {10.1109/CVPR.2017.243},
  eprint        = {1608.06993},
  pages         = {2261--2269},
  title         = {{Densely connected convolutional networks}},
  volume        = {2017-Janua},
  year          = {2017}
}

@article{Ji2018,
  abstract  = {This study describes a novel three-dimensional (3D) convolutional neural networks (CNN) based method that automatically classifies crops from spatio-temporal remote sensing images. First, 3D kernel is designed according to the structure of multi-spectral multi-temporal remote sensing data. Secondly, the 3D CNN framework with fine-tuned parameters is designed for training 3D crop samples and learning spatio-temporal discriminative representations, with the full crop growth cycles being preserved. In addition, we introduce an active learning strategy to the CNN model to improve labelling accuracy up to a required threshold with the most efficiency. Finally, experiments are carried out to test the advantage of the 3D CNN, in comparison to the two-dimensional (2D) CNN and other conventional methods. Our experiments show that the 3D CNN is especially suitable in characterizing the dynamics of crop growth and outperformed the other mainstream methods.},
  author    = {Ji, Shunping and Zhang, Chi and Xu, Anjian and Shi, Yun and Duan, Yulin},
  doi       = {10.3390/rs10010075},
  issn      = {2072-4292},
  journal   = {Remote Sensing},
  keywords  = {3D convolution,Active learning,Convolutional neural networks,Crop classification,Multi-temporal remote sensing images},
  number    = {2},
  pages     = {75},
  publisher = {MDPI AG},
  title     = {{3D Convolutional Neural Networks for Crop Classification with Multi-Temporal Remote Sensing Images}},
  volume    = {10},
  year      = {2018}
}

@article{Ienco2019,
  abstract  = {The huge amount of data currently produced by modern Earth Observation (EO) missions has allowed for the design of advanced machine learning techniques able to support complex Land Use/Land Cover (LULC) mapping tasks. The Copernicus programme developed by the European Space Agency provides, with missions such as Sentinel-1 (S1) and Sentinel-2 (S2), radar and optical (multi-spectral) imagery, respectively, at 10 m spatial resolution with revisit time around 5 days. Such high temporal resolution allows to collect Satellite Image Time Series (SITS) that support a plethora of Earth surface monitoring tasks. How to effectively combine the complementary information provided by such sensors remains an open problem in the remote sensing field. In this work, we propose a deep learning architecture to combine information coming from S1 and S2 time series, namely TWINNS (TWIn Neural Networks for Sentinel data), able to discover spatial and temporal dependencies in both types of SITS. The proposed architecture is devised to boost the land cover classification task by leveraging two levels of complementarity, i.e., the interplay between radar and optical SITS as well as the synergy between spatial and temporal dependencies. Experiments carried out on two study sites characterized by different land cover characteristics (i.e., the Koumbia site in Burkina Faso and Reunion Island, a overseas department of France in the Indian Ocean), demonstrate the significance of our proposal.},
  author    = {Ienco, Dino and Interdonato, Roberto and Gaetano, Raffaele and {Ho Tong Minh}, Dinh},
  doi       = {10.1016/j.isprsjprs.2019.09.016},
  issn      = {09242716},
  journal   = {ISPRS Journal of Photogrammetry and Remote Sensing},
  keywords  = {Data fusion,Deep learning,Land cover classification,Satellite Image Time Series,Sentinel-1,Sentinel-2},
  pages     = {11--22},
  publisher = {Elsevier B.V.},
  title     = {{Combining Sentinel-1 and Sentinel-2 Satellite Image Time Series for land cover mapping via a multi-source deep learning architecture}},
  volume    = {158},
  year      = {2019}
}

@article{Liu2017,
  abstract      = {This paper proposes a novel deep learning framework named bidirectional-convolutional long short term memory (Bi-CLSTM) network to automatically learn the spectral-spatial feature from hyperspectral images (HSIs). In the network, the issue of spectral feature extraction is considered as a sequence learning problem, and a recurrent connection operator across the spectral domain is used to address it. Meanwhile, inspired from the widely used convolutional neural network (CNN), a convolution operator across the spatial domain is incorporated into the network to extract the spatial feature. Besides, to sufficiently capture the spectral information, a bidirectional recurrent connection is proposed. In the classification phase, the learned features are concatenated into a vector and fed to a softmax classifier via a fully-connected operator. To validate the effectiveness of the proposed Bi-CLSTM framework, we compare it with several state-of-the-art methods, including the CNN framework, on three widely used HSIs. The obtained results show that Bi-CLSTM can improve the classification performance as compared to other methods.},
  archiveprefix = {arXiv},
  author        = {Liu, Qingshan and Zhou, Feng and Hang, Renlong and Yuan, Xiaotong},
  issn          = {2072-4292},
  journal       = {Remote Sensing},
  keywords      = {bidirectional recurrent network,convolution operator,feature learning,hyperspectral image classification,long short term memory},
  number        = {12},
  pages         = {1330},
  publisher     = {MDPI AG},
  title         = {{Bidirectional-Convolutional LSTM Based Spectral-Spatial Feature Learning for Hyperspectral Image Classification}},
  volume        = {9},
  year          = {2017}
}

@article{Barbosa2020,
  abstract  = {Predicting crop yield response to management and environmental variables is a crucial step towards nutrient management optimization. With the increase in the amount of data generated by agricultural machinery, more sophisticated models are necessary to get full advantage of such data. In this work, we propose a Convolutional Neural Network (CNN) to capture relevant spatial structures of different attributes and combine them to model yield response to nutrient and seed rate management. Nine on-farm experiments on corn fields are used to construct a suitable dataset to train and test the CNN model. Four architectures combining input attributes at different stages in the network are evaluated and compared to the most commonly used predictive models. Results show a reduction in the test dataset RMSE up to 68{\%} when compared to multiple linear regression and up to 29{\%} when compared to a random forest. We also demonstrate that higher variability associated with the spatial structure of the data takes the most advantage of this framework.},
  author    = {Barbosa, Alexandre and Trevisan, Rodrigo and Hovakimyan, Naira and Martin, Nicolas F.},
  doi       = {10.1016/j.compag.2019.105197},
  issn      = {01681699},
  journal   = {Computers and Electronics in Agriculture},
  publisher = {Elsevier B.V.},
  title     = {{Modeling yield response to crop management using convolutional neural networks}},
  volume    = {170},
  year      = {2020}
}

@article{Li2017,
  abstract  = {Recent research has shown that using spectral-spatial information can considerably improve the performance of hyperspectral image (HSI) classification. HSI data is typically presented in the format of 3D cubes. Thus, 3D spatial filtering naturally offers a simple and effective method for simultaneously extracting the spectral-spatial features within such images. In this paper, a 3D convolutional neural network (3D-CNN) framework is proposed for accurate HSI classification. The proposed method views the HSI cube data altogether without relying on any preprocessing or post-processing, extracting the deep spectral-spatial-combined features effectively. In addition, it requires fewer parameters than other deep learning-based methods. Thus, the model is lighter, less likely to over-fit, and easier to train. For comparison and validation, we test the proposed method along with three other deep learning-based HSI classification methods-namely, stacked autoencoder (SAE), deep brief network (DBN), and 2D-CNN-based methods-on three real-world HSI datasets captured by different sensors. Experimental results demonstrate that our 3D-CNN-based method outperforms these state-of-the-art methods and sets a new record.},
  author    = {Li, Ying and Zhang, Haokui and Shen, Qiang},
  doi       = {10.3390/rs9010067},
  issn      = {2072-4292},
  journal   = {Remote Sensing},
  keywords  = {2D convolutional neural networks,3D convolutional neural networks,3D structure,Deep learning,Hyperspectral image classification},
  number    = {1},
  pages     = {67},
  publisher = {MDPI AG},
  title     = {{Spectral–Spatial Classification of Hyperspectral Imagery with 3D Convolutional Neural Network}},
  volume    = {9},
  year      = {2017}
}

@article{Ioffe2015,
  abstract      = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
  archiveprefix = {arXiv},
  arxivid       = {1502.03167},
  author        = {Ioffe, Sergey and Szegedy, Christian},
  doi           = {10.1007/s13398-014-0173-7.2},
  eprint        = {1502.03167},
  issn          = {0717-6163},
  pmid          = {15003161},
  title         = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
  year          = {2015}
}

@article{Schuster1997,
  abstract = {In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported. {\textcopyright} 1997 IEEE.},
  author   = {Schuster, Mike and Paliwal, Kuldip K.},
  doi      = {10.1109/78.650093},
  issn     = {1053587X},
  journal  = {IEEE Transactions on Signal Processing},
  keywords = {Recurrent neural networks},
  number   = {11},
  pages    = {2673--2681},
  title    = {{Bidirectional recurrent neural networks}},
  volume   = {45},
  year     = {1997}
}

@article{Krizhevsky2017,
  abstract      = {In this paper, we present the approach that we applied to the medical modality classification tasks at the ImageCLEF evaluation forum. More specifically, we used the modality classification databases from the ImageCLEF competitions in 2011, 2012 and 2013, described by four visual and one textual types of features, and combinations thereof. We used local binary patterns, color and edge directivity descriptors, fuzzy color and texture histogram and scale-invariant feature transform (and its variant opponentSIFT) as visual features and the standard bag-of-words textual representation coupled with TF-IDF weighting. The results from the extensive experimental evaluation identify the SIFT and opponentSIFT features as the best performing features for modality classification. Next, the low-level fusion of the visual features improves the predictive performance of the classifiers. This is because the different features are able to capture different aspects of an image, their combination offering a more complete representation of the visual content in an image. Moreover, adding textual features further increases the predictive performance. Finally, the results obtained with our approach are the best results reported on these databases so far.},
  archiveprefix = {arXiv},
  arxivid       = {1102.0183},
  author        = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  doi           = {10.1145/3065386},
  eprint        = {1102.0183},
  issn          = {00010782},
  journal       = {Communications of the ACM},
  keywords      = {Feature fusion,Image modality classification,Visual image descriptors},
  number        = {6},
  pages         = {84--90},
  pmid          = {24997992},
  title         = {{ImageNet classification with deep convolutional neural networks}},
  volume        = {60},
  year          = {2017}
}

@article{Szegedy2015,
  abstract      = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  archiveprefix = {arXiv},
  arxivid       = {1409.4842},
  author        = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  doi           = {10.1109/CVPR.2015.7298594},
  eprint        = {1409.4842},
  issn          = {10636919},
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  pages         = {1--9},
  pmid          = {24920543},
  title         = {{Going deeper with convolutions}},
  volume        = {07-12-June},
  year          = {2015}
}

@article{Zeiler2014,
  abstract      = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  archiveprefix = {arXiv},
  arxivid       = {1311.2901},
  author        = {Zeiler, Matthew D. and Fergus, Rob},
  eprint        = {1311.2901},
  issn          = {16113349},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  number        = {PART 1},
  pages         = {818--833},
  pmid          = {26353135},
  title         = {{Visualizing and understanding convolutional networks}},
  volume        = {8689 LNCS},
  year          = {2014}
}

@inproceedings{Graves2005,
  abstract  = {In this paper, we present bidirectional Long Short Term Memory (LSTM) networks, and a modified, full gradient version of the LSTM learning algorithm. We evaluate Bidirectional LSTM (BLSTM) and several other network architectures on the benchmark task of framewise phoneme classification, using the TIMIT database. Our main findings are that bidirectional networks outperform unidirectional ones, and Long Short Term Memory (LSTM) is much faster and also more accurate than both standard Recurrent Neural Nets (RNNs) and time-windowed Multilayer Perceptrons (MLPs). Our results support the view that contextual information is crucial to speech processing, and suggest that BLSTM is an effective architecture with which to exploit it. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
  author    = {Graves, Alex and Schmidhuber, J{\"{u}}rgen},
  booktitle = {Neural Networks},
  doi       = {10.1016/j.neunet.2005.06.042},
  issn      = {08936080},
  number    = {5-6},
  pages     = {602--610},
  pmid      = {16112549},
  title     = {{Framewise phoneme classification with bidirectional LSTM and other neural network architectures}},
  volume    = {18},
  year      = {2005}
}

@article{Schmidhuber2014,
  abstract      = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  archiveprefix = {arXiv},
  arxivid       = {1404.7828},
  author        = {Schmidhuber, Juergen},
  doi           = {10.1016/j.neunet.2014.09.003},
  eprint        = {1404.7828},
  issn          = {08936080},
  journal       = {Neural Networks},
  pages         = {85--117},
  pmid          = {1000198498},
  title         = {{Deep Learning in Neural Networks: An Overview}},
  volume        = {61},
  year          = {2014}
}

@inproceedings{paszke2017automatic,
  author    = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  booktitle = {NIPS-W},
  title     = {{Automatic differentiation in PyTorch}},
  year      = {2017}
}

@article{Hochreiter1997,
  abstract      = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  archiveprefix = {arXiv},
  arxivid       = {1206.2944},
  author        = {Hochreiter, Sepp and Schmidhuber, J{\"{u}}rgen},
  doi           = {10.1162/neco.1997.9.8.1735},
  eprint        = {1206.2944},
  issn          = {0899-7667},
  journal       = {Neural Computation},
  number        = {8},
  pages         = {1735--1780},
  pmid          = {9377276},
  title         = {{Long Short-Term Memory}},
  volume        = {9},
  year          = {1997}
}

@inproceedings{Gers2000,
  abstract      = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  author        = {Gers, F.A. and Schmidhuber, J.},
  booktitle     = {Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium},
  doi           = {10.1109/IJCNN.2000.861302},
  eprint        = {arXiv:1011.1669v3},
  issn          = {1098-6596},
  pages         = {189--194 vol.3},
  pmid          = {25246403},
  publisher     = {IEEE},
  title         = {{Recurrent nets that time and count}},
  year          = {2000}
}

@article{Srivastava2014a,
  abstract      = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  archiveprefix = {arXiv},
  arxivid       = {1102.4807},
  author        = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  doi           = {10.1214/12-AOS1000},
  eprint        = {1102.4807},
  issn          = {15337928},
  journal       = {Journal of Machine Learning Research},
  keywords      = {deep learning,model combination,neural networks,regularization},
  pages         = {1929--1958},
  pmid          = {23285570},
  title         = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
  volume        = {15},
  year          = {2014}
}

@article{Bergstra2012,
  abstract      = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
  archiveprefix = {arXiv},
  arxivid       = {1504.05070},
  author        = {Bergstra, James and Bengio, Yoshua},
  doi           = {10.1162/153244303322533223},
  eprint        = {1504.05070},
  issn          = {1532-4435},
  journal       = {Journal of Machine Learning Research},
  keywords      = {deep learning,global optimization,model selection,neural networks,response surface modeling},
  pages         = {281--305},
  pmid          = {18244602},
  title         = {{Random Search for Hyper-Parameter Optimization}},
  volume        = {13},
  year          = {2012}
}

@article{Kingma2015,
  abstract      = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  arxivid       = {1412.6980},
  author        = {Kingma, Diederik P. and Ba, Jimmy},
  doi           = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
  eprint        = {1412.6980},
  issn          = {09252312},
  journal       = {Iclr},
  pages         = {1--15},
  pmid          = {172668},
  title         = {{Adam: A Method for Stochastic Optimization}},
  year          = {2014}
}

@incollection{Narra2020,
  abstract  = {Precision Agriculture and Smart Farming are increasingly important concepts in agriculture. While the first is mainly related to crop production, the latter is more general, which also involves the carbon capture capacity of crop fields (Carbon Farming), as well as optimization of the farming costs taking into account the dynamics of market prices. In this paper we present our recent work in building a web-based decision support system for farmers to help them comply with these trends and requirements. The system is based on the Oskari platform, developed in Finland for the visualization and analysis of geospatial data. Our main focus so far has been in developing tools for Big Data and Deep Learning based modelling which will form the analytical engine of the decision support platform.We first give an overview on the various applications of deep learning in crop production. We also present our recent results on within-field crop yield prediction using a Convolutional Neural Network (CNN) model. The model is based on multispectral data acquired using UAVs during the growth season. The results indicate that both the crop yield and the prediction error have significant within-field variance, emphasizing the importance of developing field-wise modelling tools as a part of a decision support platform for farmers. Finally, we present the general architecture of the overall decision support platform currently under development.},
  author    = {Narra, Nathaniel and Nevavuori, Petteri and Linna, Petri and Lipping, Tarmo},
  booktitle = {Information Modelling and Knowledge Bases XXXI},
  doi       = {10.3233/FAIA200014},
  pages     = {175 -- 185},
  publisher = {IOS Press},
  title     = {{A Data Driven Approach to Decision Support in Farming}},
  volume    = {321},
  year      = {2020}
}

@inproceedings{Glorot2010,
  abstract  = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. Copyright 2010 by the authors.},
  author    = {Glorot, Xavier and Bengio, Yoshua},
  booktitle = {Journal of Machine Learning Research},
  issn      = {15324435},
  pages     = {249--256},
  title     = {{Understanding the difficulty of training deep feedforward neural networks}},
  volume    = {9},
  year      = {2010}
}

@article{Zeiler2012,
  abstract      = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
  archiveprefix = {arXiv},
  arxivid       = {1212.5701},
  author        = {Zeiler, Matthew D},
  doi           = {http://doi.acm.org.ezproxy.lib.ucf.edu/10.1145/1830483.1830503},
  eprint        = {1212.5701},
  issn          = {09252312},
  keywords      = {Gradient Descent,Index Terms— Adaptive Learning Rates,Machine Learn-ing,Neural Networks},
  title         = {{ADADELTA: An Adaptive Learning Rate Method}},
  year          = {2012}
}

@article{Graves2013,
  abstract      = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
  archiveprefix = {arXiv},
  arxivid       = {1308.0850},
  author        = {Graves, Alex},
  doi           = {10.1145/2661829.2661935},
  eprint        = {1308.0850},
  issn          = {18792782},
  pages         = {1--43},
  pmid          = {23459267},
  title         = {{Generating Sequences With Recurrent Neural Networks}},
  year          = {2013}
}

@manual{skorch,
  author = {Tietz, Marian and Fan, Thomas J and Nouri, Daniel and Bossan, Benjamin and skorch Developers},
  note   = {\url{https://skorch.readthedocs.io/en/stable/}},
  title  = {{skorch: A scikit-learn compatible neural network library that wraps PyTorch}},
  year   = {2017}
}

@article{Sun2020,
  abstract = {Potato is the largest non-cereal food crop in the world. Timely estimation of end-of-season tuber production using in-season information can inform sustainable agricultural management decisions that increase productivity while reducing impacts on the environment. Recently, unmanned aerial vehicles (UAVs) have become increasingly popular in precision agriculture due to their flexibility in data acquisition and improved spatial and spectral resolutions. In addition, compared with natural color and multispectral imagery, hyperspectral data can provide higher spectral fidelity which is important for modelling crop traits. In this study, we conducted end-of-season potato tuber yield and tuber set predictions using in-season UAV-based hyperspectral images and machine learning. Specifically, six mainstream machine learning models, i.e., ordinary least square (OLS), ridge regression, partial least square regression (PLSR), support vector regression (SVR), random forest (RF), and adaptive boosting (AdaBoost), were developed and compared across potato research plots with different irrigation rates at the University of Wisconsin Hancock Agricultural Research Station. Our results showed that the tuber set could be better predicted than the tuber yield, and using the multi-temporal hyperspectral data improved the model performance. Ridge achieved the best performance for predicting tuber yield (R2 = 0.63) while Ridge and PLSR had similar performance for predicting tuber set (R2 = 0.69). Our study demonstrated that hyperspectral imagery and machine learning have good potential to help potato growers efficiently manage their irrigation practices.},
  author   = {Sun, Chen and Feng, Luwei and Zhang, Zhou and Ma, Yuchi and Crosby, Trevor and Naber, Mack and Wang, Yi},
  doi      = {10.3390/s20185293},
  issn     = {14248220},
  journal  = {Sensors (Switzerland)},
  keywords = {Hyperspectral imaging,Machine learning,Tuber set,Tuber yield,Unmanned aerial vehicles},
  number   = {18},
  pages    = {1--13},
  pmid     = {32947919},
  title    = {{Prediction of end-of-season tuber yield and tuber set in potatoes using in-season uav-based hyperspectral imagery and machine learning}},
  volume   = {20},
  year     = {2020}
}

@article{Lee2020,
  abstract  = {Crop nitrogen (N) needs to be accurately predicted to allow farmers to effectively match the N supply to the crop N demand during crop growth in order to minimize environmental impacts as excess N could seep into the water supplies around the field. The objective of this study is to use Unmanned Aerial Vehicle (UAV) multispectral MicaSense imagery validated with hyperspectral ground measurements to predict canopy nitrogen weight (g/m2) of wheat and cornfields in Ontario. A simple linear regression was established to predict the canopy nitrogen weight from various vegetation indices (VI). Ratio Vegetation Index (RVI) performed the best out of all the tested vegetation indices, with an R 2 of 0.93 for the wheat fields and 0.83 for the corn fields. RVI estimation was also consistent throughout the growing season, which is optimal in precision agriculture. Once applied the RVI-based regression model to the UAV imagery, the best RMSE was 0.95 g/m2 for the wheat McColl field using the image of May 24th and 0.66 g/m2 for the corn Jack North field using the image of June 7th. Such information for accurately predicting nitrogen is important for farmers as it will lead to a more efficient fertilizer application program.},
  author    = {Lee, Hwang and Wang, Jinfei and Leblon, Brigitte},
  doi       = {10.1080/07038992.2020.1788384},
  issn      = {17127971},
  journal   = {Canadian Journal of Remote Sensing},
  number    = {4},
  pages     = {1--19},
  publisher = {Taylor {\&} Francis},
  title     = {{Intra-Field Canopy Nitrogen Retrieval from Unmanned Aerial Vehicle Imagery for Wheat and Corn Fields}},
  volume    = {46},
  year      = {2020}
}

@article{Fu2020,
  abstract = {Leaf area index (LAI) and leaf dry matter (LDM) are important indices of crop growth. Real-time, nondestructive monitoring of crop growth is instructive for the diagnosis of crop growth and prediction of grain yield. Unmanned aerial vehicle (UAV)-based remote sensing is widely used in precision agriculture due to its unique advantages in flexibility and resolution. This study was carried out on wheat trials treated with different nitrogen levels and seeding densities in three regions of Jiangsu Province in 2018-2019. Canopy spectral images were collected by the UAV equipped with a multi-spectral camera during key wheat growth stages. To verify the results of the UAV images, the LAI, LDM, and yield data were obtained by destructive sampling. We extracted the wheat canopy reflectance and selected the best vegetation index for monitoring growth and predicting yield. Simple linear regression (LR), multiple linear regression (MLR), stepwise multiple linear regression (SMLR), partial least squares regression (PLSR), artificial neural network (ANN), and random forest (RF) modeling methods were used to construct a model for wheat yield estimation. The results show that the multi-spectral camera mounted on the multi-rotor UAV has a broad application prospect in crop growth index monitoring and yield estimation. The vegetation index combined with the red edge band and the near-infrared band was significantly correlated with LAI and LDM. Machine learning methods (i.e., PLSR, ANN, and RF) performed better for predicting wheat yield. The RF model constructed by normalized difference vegetation index (NDVI) at the jointing stage, heading stage, flowering stage, and filling stage was the optimal wheat yield estimation model in this study, with an R2 of 0.78 and relative root mean square error (RRMSE) of 0.1030. The results provide a theoretical basis for monitoring crop growth with a multi-rotor UAV platform and explore a technical method for improving the precision of yield estimation.},
  author   = {Fu, Zhaopeng and Jiang, Jie and Gao, Yang and Krienke, Brian and Wang, Meng and Zhong, Kaitai and Cao, Qiang and Tian, Yongchao and Zhu, Yan and Cao, Weixing and Liu, Xiaojun},
  doi      = {10.3390/rs12030508},
  issn     = {20724292},
  journal  = {Remote Sensing},
  keywords = {Estimation,Grain yield,Leaf area index,Leaf dry matter,Machine learning,Uav multispectral image,Wheat},
  number   = {3},
  title    = {{Wheat growth monitoring and yield estimation based on multi-rotor unmanned aerial vehicle}},
  volume   = {12},
  year     = {2020}
}

@article{Messina2020,
  abstract = {Low-altitude remote sensing (RS) using unmanned aerial vehicles (UAVs) is a powerful tool in precision agriculture (PA). In that context, thermal RS has many potential uses. The surface temperature of plants changes rapidly under stress conditions, which makes thermal RS a useful tool for real-time detection of plant stress conditions. Current applications of UAV thermal RS include monitoring plant water stress, detecting plant diseases, assessing crop yield estimation, and plant phenotyping. However, the correct use and interpretation of thermal data are based on basic knowledge of the nature of thermal radiation. Therefore, aspects that are related to calibration and ground data collection, in which the use of reference panels is highly recommended, as well as data processing, must be carefully considered. This paper aims to review the state of the art of UAV thermal RS in agriculture, outlining an overview of the latest applications and providing a future research outlook.},
  author   = {Messina, Gaetano and Modica, Giuseppe},
  doi      = {10.3390/RS12091491},
  issn     = {20724292},
  journal  = {Remote Sensing},
  keywords = {Crop water stress monitoring,Plant disease detection,Precision agriculture (PA),Remote sensing (RS),Thermal UAV RS,Thermal infrared (TIR),Unmanned aerial vehicles (UAVs),Vegetation status monitoring,Yield estimation},
  number   = {9},
  title    = {{Applications of UAV thermal imagery in precision agriculture: State of the art and future research outlook}},
  volume   = {12},
  year     = {2020}
}

@article{Borra-Serrano2020,
  abstract = {Close remote sensing approaches can be used for high throughput on-field phenotyping in the context of plant breeding and biological research. Data on canopy cover (CC) and canopy height (CH) and their temporal changes throughout the growing season can yield information about crop growth and performance. In the present study, sigmoid models were fitted to multi-temporal CC and CH data obtained using RGB imagery captured with a drone for a broad set of soybean genotypes. The Gompertz and Beta functions were used to fit CC and CH data, respectively. Overall, 90.4{\%} fits for CC and 99.4{\%} fits for CH reached an adjusted R2 {\textgreater} 0.70, demonstrating good performance of the models chosen. Using these growth curves, parameters including maximum absolute growth rate, early vigor, maximum height, and senescence were calculated for a collection of soybean genotypes. This information was also used to estimate seed yield and maturity (R8 stage) (adjusted R2 = 0.51 and 0.82). Combinations of parameter values were tested to identify genotypes with interesting traits. An integrative approach of fitting a curve to a multi-temporal dataset resulted in biologically interpretable parameters that were informative for relevant traits.},
  author   = {Borra-Serrano, Irene and Swaef, Tom De and Quataert, Paul and Aper, Jonas and Saleem, Aamir and Saeys, Wouter and Somers, Ben and Rold{\'{a}}n-Ruiz, Isabel and Lootens, Peter},
  doi      = {10.3390/rs12101644},
  issn     = {20724292},
  journal  = {Remote Sensing},
  keywords = {Canopy cover,Canopy height,Close remote sensing,Curve fitting,Glycine max,Growth model,RGB},
  number   = {10},
  pages    = {1--19},
  title    = {{Closing the phenotyping gap: High resolution UAV time series for soybean growth analysis provides objective data from field trials}},
  volume   = {12},
  year     = {2020}
}

@article{Hauglin2016,
  abstract = {Invasive species can be considered a threat to biodiversity, and remote sensing has been proposed as a tool for detection and monitoring of invasive species. In this study, we test the ability to discriminate between two tree species of the same genera, using data from Landsat 8 satellite imagery, aerial images, and airborne laser scanning. Ground observations from forest stands dominated by either Norway spruce (Picea abies) or Sitka spruce (Picea sitchensis) were coupled with variables derived from each of the three sets of remote sensing data. Random forest, support vector machine, and logistic regression classification models were fit to the data, and the classification accuracy tested by performing a cross-validation. Classification accuracies were compared for different combinations of remote sensing data and classification methods. The overall classification accuracy varied from 0.53 to 0.79, with the highest accuracy obtained using logistic regression with a combination of data derived from Landsat imagery and aerial images. The corresponding kappa value was 0.58. The contribution to the classification accuracy from using airborne data in addition to Landsat imagery was not substantial in this study. The classification accuracy varied between models using data from individual Landsat images.},
  author   = {Hauglin, Marius and {\O}rka, Hans Ole},
  doi      = {10.3390/rs8050363},
  issn     = {20724292},
  journal  = {Remote Sensing},
  keywords = {Invasive species,Tree species classification},
  number   = {5},
  title    = {{Discriminating between native norway spruce and invasive sitka spruce-A comparison of multitemporal Landsat 8 imagery, aerial images and airborne laser scanner data}},
  volume   = {8},
  year     = {2016}
}

@article{Liu2019,
  abstract  = {The expected increasing availability of remote sensing satellite hyperspectral (HS) images provides an important and unique data source for Earth observation (EO). HS images are characterized by a detailed spectral sampling (i.e., very high spectral resolution) over a wide spectral wavelength range, which makes it possible to monitor land-cover dynamics at a fine spectral scale. This is due to its capability of detecting subtle spectral variations in multitemporal images associated with land-cover changes that are not detectable in traditional multispectral (MS) images because of their limited spectral resolution (i.e., sufficient for representing only abrupt, strong changes in the spectral signature, as a rule). To fully exploit the available multitemporal HS images and their rich information content in change detection (CD), it is necessary to develop advanced automatic techniques that can address the complexity of the extraction of change information in an HS space. This article provides a comprehensive overview of the CD problem in HS images, as well as a survey on the main CD techniques available for multitemporal HS images. We review both widely used methods and new techniques proposed in the recent literature. The basic concepts, categories, open issues, and challenges related to CD in HS images are discussed and analyzed in detail. Experimental results obtained using state-of-the-art approaches are shown, to illustrate relevant concepts and problems.},
  author    = {Liu, Sicong and Marinelli, Daniele and Bruzzone, Lorenzo and Bovolo, Francesca},
  doi       = {10.1109/MGRS.2019.2898520},
  issn      = {21686831},
  journal   = {IEEE Geoscience and Remote Sensing Magazine},
  number    = {2},
  pages     = {140--158},
  publisher = {IEEE},
  title     = {{A review of change detection in multitemporal hyperspectral images: Current techniques, applications, and challenges}},
  volume    = {7},
  year      = {2019}
}

@article{Ghamisi2019,
  abstract = {The recent, sharp increase in the availability of data captured by different sensors, combined with their considerable heterogeneity, poses a serious challenge for the effective and efficient processing of remotely sensed data. Such an increase in remote sensing and ancillary data sets, however, opens up the possibility of utilizing multimodal data sets in a joint manner to further improve the performance of the processing approaches with respect to applications at hand. Multisource data fusion has, therefore, received enormous attention from researchers worldwide for a wide variety of applications. Moreover, thanks to the revisit capability of several.},
  author   = {Ghamisi, Pedram and Rasti, Behnood and Yokoya, Naoto and Wang, Qunming and Hofle, Bernhard and Bruzzone, Lorenzo and Bovolo, Francesca and Chi, Mingmin and Anders, Katharina and Gloaguen, Richard and Atkinson, Peter M. and Benediktsson, Jon Atli},
  doi      = {10.1109/MGRS.2018.2890023},
  issn     = {21686831},
  journal  = {IEEE Geoscience and Remote Sensing Magazine},
  number   = {1},
  pages    = {6--39},
  title    = {{Multisource and multitemporal data fusion in remote sensing: A comprehensive review of the state of the art}},
  volume   = {7},
  year     = {2019}
}

@article{Singh2016,
  author  = {Singh, Arti and Ganapathysubramanian, Baskar and Singh, Asheesh Kumar and Sarkar, Soumik},
  doi     = {10.1016/j.tplants.2015.10.015},
  issn    = {13601385},
  journal = {Trends in Plant Science},
  number  = {2},
  pages   = {110--124},
  pmid    = {26651918},
  title   = {Machine Learning for High-Throughput Stress Phenotyping in Plants},
  volume  = {21},
  year    = {2016}
}

@article{Singh2018,
  author  = {Singh, Asheesh Kumar and Ganapathysubramanian, Baskar and Sarkar, Soumik and Singh, Arti},
  doi     = {10.1016/J.TPLANTS.2018.07.004},
  issn    = {1360-1385},
  journal = {Trends in Plant Science},
  number  = {10},
  pages   = {883--898},
  title   = {Deep Learning for Plant Stress Phenotyping: Trends and Future Perspectives},
  volume  = {23},
  year    = {2018}
}

@article{Elavarasan2018,
  author  = {Elavarasan, Dhivya and Vincent, Durai Raj and Sharma, Vishal and Zomaya, Albert Y. and Srinivasan, Kathiravan},
  doi     = {10.1016/J.COMPAG.2018.10.024},
  issn    = {0168-1699},
  journal = {Computers and Electronics in Agriculture},
  pages   = {257--282},
  title   = {Forecasting yield by integrating agrarian factors and machine learning models: A survey},
  volume  = {155},
  year    = {2018}
}

@article{Rehman2019,
  author  = {Rehman, Tanzeel U. and Mahmud, Md. Sultan and Chang, Young K. and Jin, Jian and Shin, Jaemyung},
  doi     = {10.1016/J.COMPAG.2018.12.006},
  issn    = {0168-1699},
  journal = {Computers and Electronics in Agriculture},
  pages   = {585--605},
  title   = {Current and future applications of statistical machine learning algorithms for agricultural machine vision systems},
  volume  = {156},
  year    = {2019}
}

@article{Golhani2018,
  author  = {Golhani, Kamlesh and Balasundram, Siva K. and Vadamalai, Ganesan and Pradhan, Biswajeet},
  doi     = {10.1016/J.INPA.2018.05.002},
  issn    = {2214-3173},
  journal = {Information Processing in Agriculture},
  number  = {3},
  pages   = {354--371},
  title   = {A review of neural networks in plant disease detection using hyperspectral data},
  volume  = {5},
  year    = {2018}
}

@article{Kamilaris2018b,
  author  = {Kamilaris, A. and Prenafeta-Bold{\'{u}}, F. X.},
  doi     = {10.1017/S0021859618000436},
  issn    = {14695146},
  journal = {Journal of Agricultural Science},
  number  = {June},
  pages   = {1--11},
  title   = {A review of the use of convolutional neural networks in agriculture},
  year    = {2018}
}

@article{Kamilaris2018a,
  author  = {Kamilaris, A. and Prenafeta-Bold{\'{u}}, F. X.},
  doi     = {doi.org/10.1016/j.compag.2018.02.016},
  issn    = {0168-1699},
  journal = {Computers and Electronics in Agriculture},
  pages   = {70-90},
  title   = {Deep learning in agriculture: A survey},
  volume  = {147},
  year    = {2018}
}

@article{Patricio2018,
  author  = {Patr{\'{i}}cio, Diego In{\'{a}}cio and Rieder, Rafael},
  doi     = {10.1016/j.compag.2018.08.001},
  issn    = {01681699},
  journal = {Computers and Electronics in Agriculture},
  pages   = {69--81},
  title   = {Computer vision and artificial intelligence in precision agriculture for grain crops: A systematic review},
  volume  = {153},
  year    = {2018}
}

@incollection{Bottou1998,
  address   = {New York, NY, USA},
  author    = {Bottou, L{\'{e}}on},
  chapter   = {On-line Le},
  editor    = {Saad, David},
  pages     = {9--42},
  publisher = {Cambridge University Press},
  title     = {{On-line Learning in Neural Networks}},
  year      = {1998}
}

@article{Chlingaryan2018,
  author        = {Chlingaryan, Anna and Sukkarieh, Salah and Whelan, Brett},
  doi           = {10.1016/j.compag.2018.05.012},
  issn          = {01681699},
  journal       = {Computers and Electronics in Agriculture},
  keywords      = {Decision making,Features extraction,Information fusion,Predictive modelling,Vegetation indices,review,yield},
  mendeley-tags = {review,yield},
  number        = {November 2017},
  pages         = {61--69},
  publisher     = {Elsevier},
  title         = {{Machine learning approaches for crop yield prediction and nitrogen status estimation in precision agriculture: A review}},
  volume        = {151},
  year          = {2018}
}

@article{Chunjing2017,
  author   = {Chunjing, Yao and Yueyao, Zhang and Yaxuan, Zhang and Liu, Haibo},
  doi      = {10.5194/isprs-archives-XLII-2-W7-989-2017},
  issn     = {2194-9034},
  journal  = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  keywords = {Agricultural remote sensing,Convolutional Neural Network,Crop Classification,Deep Learning,High resolution image},
  pages    = {989--992},
  title    = {{Application of Convolutional Neural Network in Classification of High Resolution Agricultural Remote Sensing Images}},
  volume   = {XLII-2/W7},
  year     = {2017}
}

@misc{Glorot2011,
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  issn   = {1938-7228},
  pages  = {315--323},
  title  = {{Deep Sparse Rectifier Neural Networks}},
  year   = {2011}
}

@misc{Hinton2014,
  author = {Hinton, Geoffrey and Srivastava, Ni@sh and Swersky, Kevin},
  title  = {{Neural Networks for Machine Learning Lecture 6a: Overview of minibatch gradient descent}},
  year   = {2014}
}

@article{Jiang2004,
  author        = {Jiang, D. and Yang, X. and Clinton, N. and Wang, N.},
  doi           = {10.1080/0143116031000150068},
  issn          = {0143-1161},
  journal       = {International Journal of Remote Sensing},
  keywords      = {yield},
  mendeley-tags = {yield},
  number        = {9},
  pages         = {1723--1732},
  title         = {{An artificial neural network model for estimating crop yields using remotely sensed information}},
  volume        = {25},
  year          = {2004}
}

@article{Karpathy2017,
  author    = {Karpathy, Andrej and Fei-Fei, Li},
  doi       = {10.1109/TPAMI.2016.2598339},
  issn      = {0162-8828},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number    = {4},
  pages     = {664--676},
  publisher = {IEEE Computer Society},
  title     = {{Deep Visual-Semantic Alignments for Generating Image Descriptions}},
  volume    = {39},
  year      = {2017}
}

@article{Kaul2005,
  author        = {Kaul, Monisha and Hill, Robert L. and Walthall, Charles},
  doi           = {10.1016/j.agsy.2004.07.009},
  issn          = {0308521X},
  journal       = {Agricultural Systems},
  keywords      = {Artificial neural network,Multiple regression,Soil fertility,Yield prediction models,corn,soybean,yield},
  mendeley-tags = {corn,soybean,yield},
  number        = {1},
  pages         = {1--18},
  title         = {{Artificial neural networks for corn and soybean yield prediction}},
  volume        = {85},
  year          = {2005}
}

@article{Khanal2018,
  author        = {Khanal, Sami and Fulton, John and Klopfenstein, Andrew and Douridas, Nathan and Shearer, Scott},
  doi           = {10.1016/j.compag.2018.07.016},
  issn          = {01681699},
  journal       = {Computers and Electronics in Agriculture},
  keywords      = {DEM,Mapping,Remote sensing,Soil,Yield,yield},
  mendeley-tags = {yield},
  number        = {August},
  pages         = {213--225},
  publisher     = {Elsevier},
  title         = {{Integration of high resolution remotely sensed data and machine learning techniques for spatial prediction of soil properties and corn yield}},
  volume        = {153},
  year          = {2018}
}

@article{Krizhevsky2017a,
  archiveprefix = {arXiv},
  arxivid       = {1102.0183},
  author        = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  doi           = {10.1145/3065386},
  eprint        = {1102.0183},
  issn          = {00010782},
  journal       = {Communications of the ACM},
  keywords      = {Feature fusion,Image modality classification,Visual image descriptors},
  number        = {6},
  pages         = {84--90},
  pmid          = {24997992},
  title         = {{ImageNet classification with deep convolutional neural networks}},
  volume        = {60},
  year          = {2017}
}

@article{LeCun1998b,
  archiveprefix = {arXiv},
  arxivid       = {1102.0183},
  author        = {LeCun, Yann and Bottou, Leon and Bengio, Yoshua and Haffner, Patrick},
  doi           = {10.1109/5.726791},
  eprint        = {1102.0183},
  issn          = {00189219},
  journal       = {Proceedings of the IEEE},
  keywords      = {character recognition,convolutional neural networks,document recog-,finite state transducers,gradient-based learning,graph,machine learning,neural networks,nition,ocr,optical,transformer networks},
  number        = {11},
  pages         = {2278--2324},
  pmid          = {15823584},
  title         = {{Gradient Based Learning Applied to Document Recognition}},
  volume        = {86},
  year          = {1998}
}

@article{Matikainen2017,
  author  = {Matikainen, L. and Karila, K. and Hyypp{\"{a}}, J. and Puttonen, E. and Litkey, P. and Ahokas, E.},
  doi     = {10.5194/isprs-archives-XLII-3-W3-119-2017},
  issn    = {2194-9034},
  journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  pages   = {119--122},
  title   = {{Feasibility of Multispectral Airborne Laser Scanning for Land Cover Classification, Road Mapping and Map Updating}},
  volume  = {XLII-3/W3},
  year    = {2017}
}

@article{Milioto2017b,
  archiveprefix = {arXiv},
  arxivid       = {1709.06764},
  author        = {Milioto, Andres and Lottes, Philipp and Stachniss, Cyrill},
  doi           = {10.1007/978-3-319-48036-7{\_}9},
  eprint        = {1709.06764},
  issn          = {21945357},
  journal       = {Advances in Intelligent Systems and Computing},
  keywords      = {Agriculture robotics,Classification,Convolutional neural networks,Segmentation},
  pages         = {105--121},
  title         = {{Real-time Semantic Segmentation of Crop and Weed for Precision Agriculture Robots Leveraging Background Knowledge in CNNs}},
  volume        = {531},
  year          = {2017}
}

@article{Miyoshi2017,
  author  = {Miyoshi, G. T. and Imai, N. N. and de Moraes, M. V. A. and Tommaselli, A. M. G. and N{\"{a}}si, R.},
  doi     = {10.5194/isprs-archives-XLII-3-W3-123-2017},
  issn    = {2194-9034},
  journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  pages   = {123--128},
  title   = {{Time Series of Images to Improve Tree Species Classification}},
  volume  = {XLII-3/W3},
  year    = {2017}
}

@article{Nasi2017,
  author  = {N{\"{a}}si, R. and Viljanen, N. and Kaivosoja, J. and Hakala, T. and Pand{\v{z}}i{\'{c}}, M. and Markelin, L. and Honkavaara, E.},
  doi     = {10.5194/isprs-archives-XLII-3-W3-137-2017},
  issn    = {2194-9034},
  journal = {ISPRS - International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
  pages   = {137--141},
  title   = {{Assessment of Various Remote Sensing Technologies in Biomass and Nitrogen Content Estimation Using an Agricultural Test Field}},
  volume  = {XLII-3/W3},
  year    = {2017}
}

@article{OGrady2017a,
  author    = {O'Grady, Michael J. and O'Hare, Gregory M.P.},
  doi       = {10.1016/j.inpa.2017.05.001},
  issn      = {22143173},
  journal   = {Information Processing in Agriculture},
  keywords  = {Internet of things,Modelling,Sensing,Smart agriculture},
  number    = {3},
  pages     = {179--187},
  publisher = {China Agricultural University},
  title     = {{Modelling the smart farm}},
  volume    = {4},
  year      = {2017}
}

@article{Pantazi2016,
  author        = {Pantazi, X. E. and Moshou, D. and Alexandridis, T. and Whetton, R. L. and Mouazen, A. M.},
  doi           = {10.1016/j.compag.2015.11.018},
  issn          = {01681699},
  journal       = {Computers and Electronics in Agriculture},
  keywords      = {Artificial neural networks,Data fusion,NDVI,Precision farming,Satellite imagery,Soil spectroscopy,wheat,yield},
  mendeley-tags = {wheat,yield},
  pages         = {57--65},
  publisher     = {Elsevier B.V.},
  title         = {{Wheat yield prediction using machine learning and advanced sensing techniques}},
  volume        = {121},
  year          = {2016}
}

@article{Russ2009b,
  author   = {Ru{\ss}, Georg},
  doi      = {10.1007/978-3-642-03067-3{\_}3},
  issn     = {03029743},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  keywords = {Data Mining,Modeling,Precision Agriculture,Regression},
  pages    = {24--37},
  title    = {{Data mining of agricultural yield data: A comparison of regression models}},
  volume   = {5633 LNAI},
  year     = {2009}
}

@article{Russakovsky2015,
  author  = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C and Fei-Fei, Li},
  doi     = {10.1007/s11263-015-0816-y},
  issn    = {1573-1405},
  journal = {International Journal of Computer Vision},
  number  = {3},
  pages   = {211--252},
  title   = {{ImageNet Large Scale Visual Recognition Challenge}},
  volume  = {115},
  year    = {2015}
}

@article{Sa2017b,
  archiveprefix = {arXiv},
  arxivid       = {1709.03329},
  author        = {Sa, Inkyu and Chen, Zetao and Popovic, Marija and Khanna, Raghav and Liebisch, Frank and Nieto, Juan and Siegwart, Roland},
  eprint        = {1709.03329},
  title         = {{weedNet: Dense Semantic Weed Classification Using Multispectral Images and MAV for Smart Farming}},
  year          = {2017}
}

@article{Tiusanen2017,
  author  = {Tiusanen, Johannes},
  journal = {K{\"{a}}yt{\"{a}}nn{\"{o}}n Maamies},
  title   = {{Aineiston k{\"{a}}sittely ja muotoilu}},
  year    = {2017}
}

@article{Wolfert2017d,
  author    = {Wolfert, Sjaak and Ge, Lan and Verdouw, Cor and Bogaardt, Marc Jeroen},
  doi       = {10.1016/j.agsy.2017.01.023},
  issn      = {0308521X},
  journal   = {Agricultural Systems},
  keywords  = {Agriculture,Business modelling,Data,Data infrastructure,Governance,Information and communication technology},
  pages     = {69--80},
  publisher = {The Authors},
  title     = {{Big Data in Smart Farming – A review}},
  volume    = {153},
  year      = {2017}
}

@article{You2017,
  author        = {You, Jiaxuan and Li, Xiaocheng and Low, Melvin and Lobell, David and Ermon, Stefano},
  journal       = {31th AAAI Conference on Artificial Intelligence},
  keywords      = {CNN,Gaussian process,LSTM,Special Track on Computational Sustainability,Yield,crop,deep learning,neural network,soybean},
  mendeley-tags = {CNN,Gaussian process,LSTM,Yield,crop,deep learning,neural network,soybean},
  pages         = {4559--4565},
  title         = {{Deep Gaussian Process for Crop Yield Prediction Based on Remote Sensing Data}},
  year          = {2017}
}

@inproceedings{Rebetez2016,
  author    = {Julien Rebetez and H{\'e}ctor F. Satiz{\'a}bal and Matteo Mota and D. Noll and Lucie B{\"u}chi and Marina Wendling and Bertrand Cannelle and Andr{\'e}s P{\'e}rez-Uribe and St{\'e}phane Burgos},
  booktitle = {24th European Symposium on Artificial Neural Networks, {ESANN} 2016, Bruges, Belgium, April 27-29, 2016},
  title     = {Augmenting a convolutional neural network with local histograms - A case study in crop classification from high-resolution UAV imagery},
  year      = {2016}
}

@article{Donohue2018,
  author   = {Randall J. Donohue and Roger A. Lawes and Gonzalo Mata and David Gobbett and Jackie Ouzman},
  issn     = {0378-4290},
  journal  = {Field Crops Research},
  keywords = {Yield modelling, Canola, Wheat, MODIS},
  pages    = {79 - 90},
  title    = {Towards a national, remote-sensing-based model for predicting field-scale crop yield},
  volume   = {227},
  year     = {2018}
}

@article{Panda2010,
  author  = {Panda, Sudhanshu Sekhar and Ames, Daniel P. and Panigrahi, Suranjan},
  issn    = {2072-4292},
  journal = {Remote Sensing},
  number  = {3},
  pages   = {673--696},
  title   = {Application of Vegetation Indices for Agricultural Crop Yield Prediction Using Neural Network Techniques},
  volume  = {2},
  year    = {2010}
}

@article{Davis2006,
  author  = {Ian C. Davis and Graeme G. Wilkinson},
  doi     = {10.1117/12.689955},
  journal = {Proc.SPIE},
  number  = {},
  pages   = {6359 - 6359 - 12},
  title   = {Crop yield prediction using multipolarization radar and multitemporal visible/infrared imagery},
  volume  = {6359},
  year    = {2006}
}

@article{Rupnik2018,
  author  = {Rok Rupnik and Matjaž Kukar and Petar Vračar and Domen Košir and Darko Pevec and Zoran Bosnić},
  doi     = {doi.org/10.1016/j.compag.2018.04.001},
  issn    = {0168-1699},
  journal = {Computers and Electronics in Agriculture},
  title   = {Agro{DSS}: A decision support system for agriculture and farming},
  year    = {2018}
}

@article{Ubbens2017,
  author  = {Ubbens, Jordan R. and Stavness, Ian},
  doi     = {10.3389/fpls.2017.01190},
  issn    = {1664-462X},
  journal = {Frontiers in Plant Science},
  pages   = {1190},
  title   = {Deep Plant Phenomics: A Deep Learning Platform for Complex Plant Phenotyping Tasks},
  volume  = {8},
  year    = {2017}
}

@article{Yalcin2017,
  author  = {Hulya Yalcin},
  journal = {2017 6th International Conference on Agro-Geoinformatics},
  pages   = {1-5},
  title   = {Plant phenology recognition using deep learning: Deep-Pheno},
  year    = {2017}
}

@article{Mochida2019,
  author    = {Mochida, Keiichi and Koda, Satoru and Inoue, Komaki and Hirayama, Takashi and Tanaka, Shojiro and Nishii, Ryuei and Melgani, Farid},
  doi       = {10.1093/gigascience/giy153},
  issn      = {2047-217X},
  journal   = {GigaScience},
  number    = {1},
  publisher = {Oxford University Press},
  title     = {{Computer vision-based phenotyping for improvement of plant productivity: a machine learning perspective}},
  volume    = {8},
  year      = {2019}
}

@article{Bah2018,
  article-number = {1690},
  author         = {Bah, M Dian and Hafiane, Adel and Canals, Raphael},
  doi            = {10.3390/rs10111690},
  issn           = {2072-4292},
  journal        = {Remote Sensing},
  number         = {11},
  title          = {Deep Learning with Unsupervised Data Labeling for Weed Detection in Line Crops in {UAV} Images},
  volume         = {10},
  year           = {2018}
}

@article{Huang2018b,
  author    = {Huang, Huasheng AND Deng, Jizhong AND Lan, Yubin AND Yang, Aqing AND Deng, Xiaoling AND Zhang, Lei},
  doi       = {10.1371/journal.pone.0196302},
  journal   = {PLOS ONE},
  number    = {4},
  pages     = {1-19},
  publisher = {Public Library of Science},
  title     = {A fully convolutional network for weed mapping of unmanned aerial vehicle ({UAV}) imagery},
  volume    = {13},
  year      = {2018}
}

@article{Dyrmann2016,
  author    = {Dyrmann, Mads and Karstoft, Henrik and Midtiby, Henrik Skov},
  doi       = {10.1016/J.BIOSYSTEMSENG.2016.08.024},
  issn      = {1537-5110},
  journal   = {Biosystems Engineering},
  pages     = {72--80},
  publisher = {Academic Press},
  title     = {{Plant species classification using deep convolutional neural network}},
  volume    = {151},
  year      = {2016}
}


@inproceedings{Linna2017,
  author    = {P. {Linna} and T. {M{\"{a}}kinen} and K. {Yrj{\"{o}}nkoski}},
  booktitle = {2017 40th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)},
  doi       = {10.23919/MIPRO.2017.7973649},
  issn      = {},
  number    = {},
  pages     = {1448-1453},
  title     = {Open data based value networks: Finnish examples of public events and agriculture},
  volume    = {},
  year      = {2017}
}

@online{ESAOpenAccessHub,
  author  = {ESA},
  note    = {\url{https://scihub.copernicus.eu/}},
  title   = {{Open Access Hub}},
  urldate = {2019-12-17}
}

@online{ESASentinel2L2AAlgorithm,
  author  = {ESA},
  note    = {\url{https://earth.esa.int/web/sentinel/technical-guides/sentinel-2-msi/level-2a/algorithm}},
  title   = {{Level-2A Algorithm - Sentinel-2 MSI Technical Guide - Sentinel Online}},
  urldate = {2019-12-17}
}

@online{Ruokavirasto,
  author  = {Ruokavirasto},
  note    = {\url{https://www.ruokavirasto.fi/tietoa-meista/avointieto/tiedonluovutukset/peltolohko-usb/}},
  title   = {{Peltolohkorekisteri}},
  urldate = {2019-12-17}
}

@article{Flach2012,
  abstract = {Machine Learning brings together all the state-of-the-art methods for making sense of data. With hundreds of worked examples and explanatory figures, the book explains the principles behind these methods in an intuitive yet precise manner and will appeal to novice and experienced readers alike.},
  author   = {Flach, Peter},
  doi      = {10.1145/242224.242229},
  issn     = {9781107096394},
  pages    = {409},
  pmid     = {18292226},
  title    = {{Machine Learning: The Art and Science of Algorithms that Make Sense of Data}},
  year     = {2012}
}

@article{Buitinck2013,
  abstract      = {Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.},
  archiveprefix = {arXiv},
  arxivid       = {1309.0238},
  author        = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and Vanderplas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Ga{\"{e}}l},
  eprint        = {1309.0238},
  keywords      = {()},
  title         = {{API design for machine learning software: experiences from the scikit-learn project}},
  year          = {2013}
}

@article{Coluzzi2018,
  author  = {Coluzzi, Rosa and Imbrenda, Vito and Maria, Lanfredi and Tiziana, Simoniello},
  doi     = {10.1016/j.rse.2018.08.009},
  journal = {Remote Sensing of Environment},
  pages   = {426-443},
  title   = {A first assessment of the Sentinel-2 Level 1-C cloud mask product to support informed surface analyses},
  volume  = {217},
  year    = {2018}
}

@article{Baetens2019,
  author  = {Baetens, Louis and Desjardins, Camille and Hagolle, Olivier},
  doi     = {10.3390/rs11040433},
  journal = {Remote Sensing},
  pages   = {},
  title   = {Validation of Copernicus Sentinel-2 Cloud Masks Obtained from MAJA, Sen2Cor, and FMask Processors Using Reference Cloud Masks Generated with a Supervised Active Learning Procedure},
  volume  = {11},
  year    = {2019}
}

@article{Russwurm2018,
  abstract      = {Earth observation (EO) sensors deliver data at daily or weekly intervals. Most land use and land cover classification (LULC) approaches, however, are designed for cloud-free and mono-temporal observations. The increasing temporal capabilities of today's sensors enable the use of temporal, along with spectral and spatial features.Domains such as speech recognition or neural machine translation, work with inherently temporal data and, today, achieve impressive results by using sequential encoder-decoder structures. Inspired by these sequence-to-sequence models, we adapt an encoder structure with convolutional recurrent layers in order to approximate a phenological model for vegetation classes based on a temporal sequence of Sentinel 2 (S2) images. In our experiments, we visualize internal activations over a sequence of cloudy and non-cloudy images and find several recurrent cells that reduce the input activity for cloudy observations. Hence, we assume that our network has learned cloud-filtering schemes solely from input data, which could alleviate the need for tedious cloud-filtering as a preprocessing step for many EO approaches. Moreover, using unfiltered temporal series of top-of-atmosphere (TOA) reflectance data, our experiments achieved state-of-the-art classification accuracies on a large number of crop classes with minimal preprocessing, compared to other classification approaches.},
  archiveprefix = {arXiv},
  arxivid       = {1802.02080},
  author        = {Ru{\ss}wurm, Marc and K rner, Marco},
  doi           = {10.3390/ijgi7040129},
  eprint        = {1802.02080},
  issn          = {22209964},
  journal       = {ISPRS International Journal of Geo-Information},
  keywords      = {Crop classification,Deep learning,Land cover classification,Land use,Multi-temporal classification,Recurrent networks,Sentinel 2,Sequence encoder,Sequence-to-sequence},
  number        = {4},
  pages         = {129},
  title         = {{Multi-temporal land cover classification with sequential recurrent encoders}},
  volume        = {7},
  year          = {2018}
}

@article{Hong2019,
  abstract  = {In this paper, we aim at tackling a general but interesting cross-modality feature learning question in remote sensing community—can a limited amount of highly-discriminative (e.g., hyperspectral) training data improve the performance of a classification task using a large amount of poorly-discriminative (e.g., multispectral) data? Traditional semi-supervised manifold alignment methods do not perform sufficiently well for such problems, since the hyperspectral data is very expensive to be largely collected in a trade-off between time and efficiency, compared to the multispectral data. To this end, we propose a novel semi-supervised cross-modality learning framework, called learnable manifold alignment (LeMA). LeMA learns a joint graph structure directly from the data instead of using a given fixed graph defined by a Gaussian kernel function. With the learned graph, we can further capture the data distribution by graph-based label propagation, which enables finding a more accurate decision boundary. Additionally, an optimization strategy based on the alternating direction method of multipliers (ADMM) is designed to solve the proposed model. Extensive experiments on two hyperspectral-multispectral datasets demonstrate the superiority and effectiveness of the proposed method in comparison with several state-of-the-art methods.},
  author    = {Hong, Danfeng and Yokoya, Naoto and Ge, Nan and Chanussot, Jocelyn and Zhu, Xiao Xiang},
  doi       = {10.1016/j.isprsjprs.2018.10.006},
  issn      = {09242716},
  journal   = {ISPRS Journal of Photogrammetry and Remote Sensing},
  keywords  = {Cross-modality,Graph learning,Hyperspectral,Manifold alignment,Multispectral,Remote sensing,Semi-supervised learning},
  pages     = {193--205},
  publisher = {Elsevier B.V.},
  title     = {{Learnable manifold alignment (LeMA): A semi-supervised cross-modality learning framework for land cover and land use classification}},
  volume    = {147},
  year      = {2019}
}

@misc{LeCun1989,
  abstract  = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  author    = {LeCun, Y and Boser, B and Denker, J. S and Henderson, D and Howard, R. E and Hubbard, W and Jackel, L. D},
  copyright = {1989 Massachusetts Institute of Technology},
  issn      = {1530-888X},
  journal   = {Neural computation},
  language  = {eng},
  number    = {4},
  pages     = {541-551},
  publisher = {MIT Press},
  title     = {Backpropagation Applied to Handwritten Zip Code Recognition},
  volume    = {1},
  year      = {1989}
}


@inproceedings{Jozefowicz2015,
  abstract  = {The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU.},
  author    = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  booktitle = {32nd International Conference on Machine Learning, ICML 2015},
  pages     = {2332--2340},
  title     = {{An empirical exploration of Recurrent Network architectures}},
  volume    = {3},
  year      = {2015}
}


@article{RumelhartDavidE1986Lrbb,
  author   = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  issn     = {0028-0836},
  journal  = {Nature (London)},
  language = {eng},
  number   = {6088},
  pages    = {533-536},
  title    = {Learning representations by back-propagating errors},
  volume   = {323},
  year     = {1986}
}

@article{Zhang1990,
  abstract = {This paper proposes a parallel distributed processing model with local space-invariant interconnections, which is more readily implemented by optics and is able to classify patterns correctly, even if they have been shifted or distorted. Error backpropagation is used as a training algorithm. Computer simulation results presented indicate that the processing is effective and the network can deal with the shifted or distorted patterns. Moreover, the optical implementation architecture using matched filters for the model is discussed. {\textcopyright} 1990 Optical Society of America.},
  author   = {Zhang, Wei and Itoh, Kazuyoshi and Tanida, Jun and Ichioka, Yoshiki},
  doi      = {10.1364/ao.29.004790},
  issn     = {0003-6935},
  journal  = {Applied Optics},
  number   = {32},
  pages    = {4790},
  title    = {{Parallel distributed processing model with local space-invariant interconnections and its optical architecture}},
  volume   = {29},
  year     = {1990}
}

@incollection{Bell1995,
  abstract  = {This chapter focuses on defining the spatial variability of two soil properties that tend to be stable with time and are relevant for describing land quality in glaciated landscapes of the north‐central United States; the depth of the A‐horizon and depth to free carbonates. There are three basic approaches that have been discussed in regard to mapping soil variability for site specific farming purposes. First, county soil surveys prepared by the national cooperative soil survey program document soil variability at scales typically ranging from 1:12000 to 1:24000. A second approach uses geostatistical interpolation techniques to estimate the spatial distribution of soil properties from a network, usually a grid, of point samples. The third approach includes correlating in‐field variability with remote measurements of surface reflectance characteristics and topographic attributes defined by digital terrain analysis.},
  address   = {Madison, WI, USA},
  author    = {Bell, J. C and Butler, C. A and Thompson, J. A},
  booktitle = {Site‐Specific Management for Agricultural Systems},
  copyright = {Copyright © 1995 ASA‐CSSA‐SSSA},
  keywords  = {free carbonate ; land quality ; A‐horizon ; geostatistical interpolation techniques ; glaciated landscapes ; tates ; digital terrain analysis ; spatial variability ; nited ; site specific agricultural management ; soil properties ; north‐central},
  language  = {eng},
  pages     = {209-227},
  publisher = {American Society of Agronomy, Crop Science Society of America, Soil Science Society of America},
  title     = {Soil‐Terrain Modeling for Site‐Specific Agricultural Management},
  year      = {1995}
}

@incollection{Sundmaeker2016,
  author    = {Sundmaeker, H and Verdouw, C N and Wolfert, J and {Perez Freire}, Luis},
  booktitle = {Digitising the Industry},
  keywords  = {Friess,Innovatie,Innovation,Innovation- and Risk Management and Information Go,LEI Innovatie,LEI InnovationDigitising the Industry / Vermesan,Ovidiu,Peter,Risico- en Informatiemanagement,Risk and Information Management,River Publishers (River Publishers series in commu,WASS},
  title     = {{Internet of Food and Farm 2020}},
  year      = {2016}
}

@article{Tantalaki2019,
  abstract  = {In this paper, we provide a review of the research dedicated to applications of data science techniques, and especially machine learning techniques, in relevant agricultural systems. Big data technologies create new opportunities for data intensive decision-making. We review works in agriculture that employ the practice of big data analysis to solve various problems, which reveal opportunities and promising areas of use. The high volume and complexity of the data produced pose challenges in successfully implementing precision agriculture. Machine learning seems promising to cope with agricultural big data, but needs to reinvent itself to meet existing challenges.},
  author    = {Tantalaki, Nicoleta and Souravlas, Stavros and Roumeliotis, Manos},
  doi       = {10.1080/10496505.2019.1638264},
  issn      = {15404722},
  journal   = {Journal of Agricultural and Food Information},
  keywords  = {Big data,machine learning,precision agriculture,real-time analytics,smart farming},
  number    = {4},
  pages     = {344--380},
  publisher = {Routledge},
  title     = {{Data-Driven Decision Making in Precision Agriculture: The Rise of Big Data in Agricultural Systems}},
  volume    = {20},
  year      = {2019}
}

@article{Tsouros2019,
  abstract = {Emerging technologies such as Internet of Things (IoT) can provide significant potential in Smart Farming and Precision Agriculture applications, enabling the acquisition of real-time environmental data. IoT devices such as Unmanned Aerial Vehicles (UAVs) can be exploited in a variety of applications related to crops management, by capturing high spatial and temporal resolution images. These technologies are expected to revolutionize agriculture, enabling decision-making in days instead of weeks, promising significant reduction in cost and increase in the yield. Such decisions enable the effective application of farm inputs, supporting the four pillars of precision agriculture, i.e., apply the right practice, at the right place, at the right time and with the right quantity. However, the actual proliferation and exploitation of UAVs in Smart Farming has not been as robust as expected mainly due to the challenges confronted when selecting and deploying the relevant technologies, including the data acquisition and image processing methods. The main problem is that still there is no standardized workflow for the use of UAVs in such applications, as it is a relatively new area. In this article, we review the most recent applications of UAVs for Precision Agriculture. We discuss the most common applications, the types of UAVs exploited and then we focus on the data acquisition methods and technologies, appointing the benefits and drawbacks of each one. We also point out the most popular processing methods of aerial imagery and discuss the outcomes of each method and the potential applications of each one in the farming operations.},
  author   = {Tsouros, Dimosthenis C. and Bibi, Stamatia and Sarigiannidis, Panagiotis G.},
  doi      = {10.3390/info10110349},
  issn     = {20782489},
  journal  = {Information (Switzerland)},
  keywords = {Image processing,IoT,Precision Agriculture,Remote sensing,Review,Smart Farming,UAS,UAV,Unmanned Aerial System,Unmanned Aerial Vehicle},
  number   = {11},
  title    = {{A review on UAV-based applications for precision agriculture}},
  volume   = {10},
  year     = {2019}
}

@article{Triantafyllou2019,
  abstract = {Smart Farming is a development that emphasizes on the use of modern technologies in the cyber-physical field management cycle. Technologies such as the Internet of Things (IoT) and Cloud Computing have accelerated the digital transformation of the conventional agricultural practices promising increased production rate and product quality. The adoption of smart farming though is hampered because of the lack of models providing guidance to practitioners regarding the necessary components that constitute IoT-based monitoring systems. To guide the process of designing and implementing Smart farming monitoring systems, in this paper we propose a generic reference architecture model, taking also into consideration a very important non-functional requirement, the energy consumption restriction. Moreover, we present and discuss the technologies that incorporate the seven layers of the architecture model that are the Sensor Layer, the Link Layer, the Encapsulation Layer, the Middleware Layer, the Configuration Layer, the Management Layer and the Application Layer. Furthermore, the proposed Reference Architecture model is exemplified in a real-world application for surveying Saffron agriculture in Kozani, Greece.},
  author   = {Triantafyllou, Anna and Sarigiannidis, Panagiotis and Bibi, Stamatia},
  doi      = {10.3390/info10110348},
  issn     = {20782489},
  journal  = {Information (Switzerland)},
  keywords = {Cloud Computing,Communication technologies,Internet of Things,Precision agriculture,Smart farming,Wireless Sensor Networks},
  number   = {11},
  title    = {{Precision agriculture: A remote sensing monitoring system architecture}},
  volume   = {10},
  year     = {2019}
}

@article{Zamora-Izquierdo2019,
  abstract = {Precision Agriculture (PA), as the integration of information, communication and control technologies in agriculture, is growing day by day. The Internet of Things (IoT) and cloud computing paradigms offer advances to enhance PA connectivity. Nevertheless, their usage in this field is usually limited to specific scenarios of high cost, and they are not adapted to semi-arid conditions, or do not cover all PA management in an efficient way. For this reason, we propose a flexible platform able to cope with soilless culture needs in full recirculation greenhouses using moderately saline water. It is based on exchangeable low-cost hardware and supported by a three-tier open source software platform at local, edge and cloud planes. At the local plane, Cyber-Physical Systems (CPS) interact with crop devices to gather data and perform real-time atomic control actions. The edge plane of the platform is in charge of monitoring and managing main PA tasks near the access network to increase system reliability against network access failures. Finally, the cloud platform collects current and past records and hosts data analytics modules in a FIWARE deployment. IoT protocols like Message Queue Telemetry Transport (MQTT) or Constrained Application Protocol (CoAP) are used to communicate with CPS, while Next Generation Service Interface (NGSI) is employed for southbound and northbound access to the cloud. The system has been completely instantiated in a real prototype in frames of the EU DrainUse project, allowing the control of a real hydroponic closed system through managing software for final farmers connected to the platform.},
  author   = {Zamora-Izquierdo, Miguel A. and Santa, Jos{\'{e}} and Mart{\'{i}}nez, Juan A. and Mart{\'{i}}nez, Vicente and Skarmeta, Antonio F.},
  doi      = {10.1016/j.biosystemseng.2018.10.014},
  issn     = {15375110},
  journal  = {Biosystems Engineering},
  keywords = {Edge Computing,Hydroponics,IoT,NGSI,Precision Agriculture,Smart Farming},
  pages    = {4--17},
  title    = {{Smart farming IoT platform based on edge and cloud computing}},
  volume   = {177},
  year     = {2019}
}

@article{Morais2019,
  abstract  = {Over the last few years, an extensive set of technologies have been systematically included in precision agriculture (PA) and also in precision viticulture (PV) practices, as tools that allow efficient monitoring of nearly any parameter to achieve sustainable crop management practices and to increase both crop yield and quality. However, many technologies and standards are not yet included on those practices. Therefore, potential benefits that may result from putting together agronomic knowledge with electronics and computer technologies are still not fully accomplished. Both emergent and established paradigms, such as the Internet of Everything (IoE), Internet of Things (IoT), cloud and fog computing, together with increasingly cheaper computing technologies – with very low power requirements and a diversity of wireless technologies, available to exchange data with increased efficiency – and intelligent systems, have evolved to a level where it is virtually possible to expeditiously create and deploy any required monitoring solution. Pushed by all of these technological trends and recent developments, data integration has emerged as the layer between crops and knowledge needed to efficiently manage it. In this paper, the mySense environment is presented, aimed to systematize data acquisition procedures to address common PA/PV issues. mySense builds over a 4-layer technological structure: sensor and sensor nodes, crop field and sensor networks, cloud services and support to front-end applications. It makes available a set of free tools based on the Do-It-Yourself (DIY) concept and enables the use of Arduino{\textregistered} and Raspberry Pi (RPi) low-cost platforms to quickly prototype a complete monitoring application. Field experiments provide compelling evidences that mySense environment represents an important step forward towards Smart Farming, by enabling the use of low-cost, fast deployment, integrated and transparent technologies to increase PA/PV monitoring applications adoption.},
  author    = {Morais, Raul and Silva, Nuno and Mendes, Jorge and Ad{\~{a}}o, Telmo and P{\'{a}}dua, Lu{\'{i}}s and L{\'{o}}pez-Riquelme, J. A. and Pav{\'{o}}n-Pulido, N. and Sousa, Joaquim Jo{\~{a}}o and Peres, Emanuel},
  doi       = {10.1016/j.compag.2019.05.028},
  issn      = {01681699},
  journal   = {Computers and Electronics in Agriculture},
  keywords  = {Data integration,Internet of things,Precision agriculture,Precision viticulture,Smart farming},
  number    = {March},
  pages     = {882--894},
  publisher = {Elsevier},
  title     = {{mySense: A comprehensive data management environment to improve precision agriculture practices}},
  volume    = {162},
  year      = {2019}
}

@article{Klerkx2019,
  abstract  = {While there is a lot of literature from a natural or technical sciences perspective on different forms of digitalization in agriculture (big data, internet of things, augmented reality, robotics, sensors, 3D printing, system integration, ubiquitous connectivity, artificial intelligence, digital twins, and blockchain among others), social science researchers have recently started investigating different aspects of digital agriculture in relation to farm production systems, value chains and food systems. This has led to a burgeoning but scattered social science body of literature. There is hence lack of overview of how this field of study is developing, and what are established, emerging, and new themes and topics. This is where this article aims to make a contribution, beyond introducing this special issue which presents seventeen articles dealing with social, economic and institutional dynamics of precision farming, digital agriculture, smart farming or agriculture 4.0. An exploratory literature review shows that five thematic clusters of extant social science literature on digitalization in agriculture can be identified: 1) Adoption, uses and adaptation of digital technologies on farm; 2) Effects of digitalization on farmer identity, farmer skills, and farm work; 3) Power, ownership, privacy and ethics in digitalizing agricultural production systems and value chains; 4) Digitalization and agricultural knowledge and innovation systems (AKIS); and 5) Economics and management of digitalized agricultural production systems and value chains. The main contributions of the special issue articles are mapped against these thematic clusters, revealing new insights on the link between digital agriculture and farm diversity, new economic, business and institutional arrangements both on-farm, in the value chain and food system, and in the innovation system, and emerging ways to ethically govern digital agriculture. Emerging lines of social science enquiry within these thematic clusters are identified and new lines are suggested to create a future research agenda on digital agriculture, smart farming and agriculture 4.0. Also, four potential new thematic social science clusters are also identified, which so far seem weakly developed: 1) Digital agriculture socio-cyber-physical-ecological systems conceptualizations; 2) Digital agriculture policy processes; 3) Digitally enabled agricultural transition pathways; and 4) Global geography of digital agriculture development. This future research agenda provides ample scope for future interdisciplinary and transdisciplinary science on precision farming, digital agriculture, smart farming and agriculture 4.0.},
  author    = {Klerkx, Laurens and Jakku, Emma and Labarthe, Pierre},
  doi       = {10.1016/j.njas.2019.100315},
  issn      = {22121307},
  journal   = {NJAS - Wageningen Journal of Life Sciences},
  keywords  = {Agricultural knowledge and innovation systems,Data science,Digital social science,Digitalization,Precision agriculture,Responsible research and innovation,Robotic farming},
  number    = {October},
  pages     = {100315},
  publisher = {Elsevier},
  title     = {{A review of social science on digital agriculture, smart farming and agriculture 4.0: New contributions and a future research agenda}},
  volume    = {90-91},
  year      = {2019}
}

@article{Rose2018,
  abstract = {Agriculture is undergoing a new technology revolution supported by policy-makers around the world. While smart technologies, such as Artificial Intelligence, robotics, and the Internet of Things, could play an important role in achieving enhanced productivity and greater eco-efficiency, critics have suggested that the consideration of social implications is being side-lined. Research illustrates that some agricultural practitioners are concerned about using certain smart technologies. Indeed, some studies argue that agricultural societies may be changed, or “re-scripted,” in undesirable ways, and there is precedent to suggest that wider society may be concerned about radical new agricultural technologies. We therefore encourage policy-makers, funders, technology companies, and researchers to consider the views of both farming communities and wider society. In agriculture, the concept of responsible innovation has not been widely considered, although two recent papers have made useful suggestions. We build on these interventions by arguing that key dimensions of responsible innovation—anticipation, inclusion, reflexivity, and responsiveness—should be applied to this fourth agricultural revolution. We argue, however, that ideas of responsible innovation should be further developed in order to make them relevant and robust for emergent agri-tech, and that frameworks should be tested in practice to see if they can actively shape innovation trajectories. In making suggestions on how to construct a more comprehensive framework for responsible innovation in sustainable agriculture, we call for: (i) a more systemic approach that maps and attends to the wider ecology of innovations associated with this fourth agricultural revolution; (ii) a broadening of notions of “inclusion” in responsible innovation to account better for diverse and already existing spaces of participation in agri-tech, and (iii) greater testing of frameworks in practice to see if they are capable of making innovation processes more socially responsible.},
  author   = {Rose, David Christian and Chilvers, Jason},
  doi      = {10.3389/fsufs.2018.00087},
  issn     = {2571581X},
  journal  = {Frontiers in Sustainable Food Systems},
  keywords = {agri-tech,artificial intelligence,inclusion,publics,responsible innovation,smart farming,sustainable intensification,technology},
  number   = {December},
  pages    = {1--7},
  title    = {{Agriculture 4.0: Broadening Responsible Innovation in an Era of Smart Farming}},
  volume   = {2},
  year     = {2018}
}

@article{Unal2020,
  abstract = {Smart farming is a new concept that makes agriculture more efficient and effective by using advanced information technologies. The latest advancements in connectivity, automation, and artificial intelligence enable farmers better to monitor all procedures and apply precise treatments determined by machines with superhuman accuracy. Farmers, data scientists and, engineers continue to work on techniques that allow optimizing the human labor required in farming. With valuable information resources improving day by day, smart farming turns into a learning system and becomes even smarter. Deep learning is a type of machine learning method, using artificial neural network principles. The main feature by which deep learning networks are distinguished from neural networks is their depth and that feature makes them capable of discovering latent structures within unlabeled, unstructured data. Deep learning networks that do not need human intervention while performing automatic feature extraction have a significant advantage over previous algorithms. The focus of this study is to explore the advantages of using deep learning in agricultural applications. This bibliography reviews the potential of using deep learning techniques in agricultural industries. The bibliography contains 120 papers from the database of the Science Citation Index on the subject that were published between 2016 and 2019. These studies have been retrieved from 39 scientific journals. The papers are classified into the following categories as disease detection, plant classification, land cover identification, precision livestock farming, pest recognition, object recognition, smart irrigation, phenotyping, and weed detection.},
  author   = {Unal, Zeynep},
  doi      = {10.1109/ACCESS.2020.3000175},
  issn     = {21693536},
  journal  = {IEEE Access},
  keywords = {Machine learning,artificial neural networks,internet of things,precision agriculture},
  pages    = {105587--105609},
  title    = {{Smart Farming Becomes even Smarter with Deep Learning - A Bibliographical Analysis}},
  volume   = {8},
  year     = {2020}
}

@book{Syvajarvi2016,
  author    = {Syv{\"{a}}j{\"{a}}rvi, Jouko},
  keywords  = {Machine learning,artificial neural networks,internet of things,precision agriculture},
  pages     = {105587--105609},
  publisher = {Suomen Maatalouden Laskentakeskus Oy},
  title     = {{Reik{\"{a}}korteista digiaikaan: Maatalouden Las\-ken\-ta\-kes\-kus Oy 30 vuot\-ta, tie\-to\-jen\-k{\"{a}}\-sit\-te\-ly{\"{a}} 58 vuot\-ta}},
  year      = {2016}
}

@article{Xu2019,
  abstract  = {Agro-meteorological condition plays a fundamental role in crop production. For a specific region, the comprehensive effects of multiple meteorological factors are important indicators for the climatic suitability of the crops. To evaluate the synthetic effects, an integrated climatic assessment indicator (ICAI) are developed in Jiangsu Province, China. A newly produced meteorological assimilation driving datasets (CMADS V1.0) combined with observation data are used in establishing the indicator. The procedure to construct the indicator involves building statistical crop models by meteorological factors and determining the indicator values by classification. In modeling, two machine learning algorithms: Random Forest (RF) and Support Vector Machine (SVM) are compared and the classification model of RF is chosen to build ICAI due to its better performance in the independent test set. To determine a reasonable division in classification, distribution detection of climatic yield is carried out and Monte Carlo simulations are applied for the Kolmogorov–Smirnov (KS) test. The generated indicator includes three values: yield loss, normal and yield increment, with the spatial and temporal prediction accuracy from 67.86{\%} to 100{\%} in the test set for the Northern, Central and Southern Jiangsu. The ICAI are used to estimate the past climatic suitability of winter wheat and the future suitability under global warming conditions in Jiangsu Province. The results show that the climate in 1990s has more adverse effects on wheat production than the other two sub-periods in Northern and Southern Jiangsu. The adaptability of wheat production in Southern Jiangsu has improved greatly to the local environments during the past three decades. In addition, when annual temperature accelerates upwards, both possibilities of yield loss in Northern Jiangsu and yield increment in Southern Jiangsu will increase. Therefore, more concerns should be given to the North in future warming climate, while yield potential in the South may be further improved in this circumstance.},
  author    = {Xu, Xiangying and Gao, Ping and Zhu, Xinkai and Guo, Wenshan and Ding, Jinfeng and Li, Chunyan and Zhu, Min and Wu, Xuanwei},
  doi       = {10.1016/j.ecolind.2019.01.059},
  issn      = {1470160X},
  journal   = {Ecological Indicators},
  keywords  = {Agro-meteorological indicator,Random Forest,Support Vector Machine,Winter wheat,Yield prediction},
  number    = {July 2018},
  pages     = {943--953},
  publisher = {Elsevier},
  title     = {{Design of an integrated climatic assessment indicator (ICAI) for wheat production: A case study in Jiangsu Province, China}},
  volume    = {101},
  year      = {2019}
}

@article{Filippi2019,
  abstract = {Many broadacre farmers have a time series of crop yield monitor data for their fields which are often augmented with additional data, such as soil apparent electrical conductivity surveys and soil test results. In addition there are now readily available national and global datasets, such as rainfall and MODIS, which can be used to represent the crop-growing environment. Rather than analysing one field at a time as is typical in precision agriculture research, there is an opportunity to explore the value of combining data over multiple fields/farms and years into one dataset. Using these datasets in conjunction with machine learning approaches allows predictive models of crop yield to be built. In this study, several large farms in Western Australia were used as a case study, and yield monitor data from wheat, barley and canola crops from three different seasons (2013, 2014 and 2015) that covered {\~{}} 11 000 to {\~{}} 17 000 hectares in each year were used. The yield data were processed to a 10 m grid, and for each observation point associated predictor variables in space and time were collated. The data were then aggregated to a 100 m spatial resolution for modelling yield. Random forest models were used to predict crop yield of wheat, barley and canola using this dataset. Three separate models were created based on pre-sowing, mid-season and late-season conditions to explore the changes in the predictive ability of the model as more within-season information became available. These time points also coincide with points in the season when a management decision is made, such as the application of fertiliser. The models were evaluated with cross-validation using both fields and years for data splitting, and this was assessed at the field spatial resolution. Cross-validated results showed the models predicted yield relatively accurately, with a root mean square error of 0.36 to 0.42 t ha−1, and a Lin's concordance correlation coefficient of 0.89 to 0.92 at the field resolution. The models performed better as the season progressed, largely because more information about within-season data became available (e.g. rainfall). The more years of yield data that were available for a field, the better the predictions were, and future work should use a longer time-series of yield data. The generic nature of this method makes it possible to apply to other agricultural systems where yield monitor data is available. Future work should also explore the integration of more data sources into the models, focus on predicting at finer spatial resolutions within fields, and the possibility of using the yield forecasts to guide management decisions.},
  author   = {Filippi, Patrick and Jones, Edward J. and Wimalathunge, Niranjan S. and Somarathna, Pallegedara D.S.N. and Pozza, Liana E. and Ugbaje, Sabastine U. and Jephcott, Thomas G. and Paterson, Stacey E. and Whelan, Brett M. and Bishop, Thomas F.A.},
  doi      = {10.1007/s11119-018-09628-4},
  issn     = {15731618},
  journal  = {Precision Agriculture},
  keywords = {Empirical yield prediction,Feature extraction,Machine learning,Precision agriculture,Random forest,Remote sensing,Yield forecast},
  number   = {5},
  pages    = {1015--1029},
  title    = {{An approach to forecast grain crop yield using multi-layered, multi-farm data sets and machine learning}},
  volume   = {20},
  year     = {2019}
}

@article{Khanal2020,
  abstract = {Remote sensing (RS) technologies provide a diagnostic tool that can serve as an early warning system, allowing the agricultural community to intervene early on to counter potential problems before they spread widely and negatively impact crop productivity. With the recent advancements in sensor technologies, data management and data analytics, currently, several RS options are available to the agricultural community. However, the agricultural sector is yet to implement RS technologies fully due to knowledge gaps on their sufficiency, appropriateness and techno-economic feasibilities. This study reviewed the literature between 2000 to 2019 that focused on the application of RS technologies in production agriculture, ranging from field preparation, planting, and in-season applications to harvesting, with the objective of contributing to the scientific understanding on the potential for RS technologies to support decision-making within different production stages. We found an increasing trend in the use of RS technologies in agricultural production over the past 20 years, with a sharp increase in applications of unmanned aerial systems (UASs) after 2015. The largest number of scientific papers related to UASs originated from Europe (34{\%}), followed by the United States (20{\%}) and China (11{\%}). Most of the prior RS studies have focused on soil moisture and in-season crop health monitoring, and less in areas such as soil compaction, subsurface drainage, and crop grain quality monitoring. In summary, the literature highlighted that RS technologies can be used to support site-specific management decisions at various stages of crop production, helping to optimize crop production while addressing environmental quality, profitability, and sustainability.},
  author   = {Khanal, Sami and KC, Kushal and Fulton, John P and Shearer, Scott and Ozkan, Erdal},
  doi      = {10.3390/rs12223783},
  issn     = {2072-4292},
  journal  = {Remote Sensing},
  keywords = {precision agriculture,remote sensing,satellite,uas},
  number   = {22},
  pages    = {3783},
  title    = {{Remote Sensing in Agriculture—Accomplishments, Limitations, and Opportunities}},
  volume   = {12},
  year     = {2020}
}

@article{Aboutalebi2019,
  abstract  = {Significant efforts have been made recently in the application of high-resolution remote sensing imagery (i.e., sub-meter) captured by unmanned aerial vehicles (UAVs) for precision agricultural applications for high-value crops such as wine grapes. However, at such high resolution, shadows will appear in the optical imagery effectively reducing the reflectance and emission signal received by imaging sensors. To date, research that evaluates procedures to identify the occurrence of shadows in imagery produced by UAVs is limited. In this study, the performance of four different shadow detection methods used in satellite imagery was evaluated for high-resolution UAV imagery collected over a California vineyard during the Grape Remote sensing Atmospheric Profile and Evapotranspiration eXperiment (GRAPEX) field campaigns. The performance of the shadow detection methods was compared and impacts of shadowed areas on the normalized difference vegetation index (NDVI) and estimated evapotranspiration (ET) using the Two-Source Energy Balance (TSEB) model are presented. The results indicated that two of the shadow detection methods, the supervised classification and index-based methods, had better performance than two other methods. Furthermore, assessment of shadowed pixels in the vine canopy led to significant differences in the calculated NDVI and ET in areas affected by shadows in the high-resolution imagery. Shadows are shown to have the greatest impact on modeled soil heat flux, while net radiation and sensible heat flux are less affected. Shadows also have an impact on the modeled Bowen ratio (ratio of sensible to latent heat) which can be used as an indicator of vine stress level.},
  author    = {Aboutalebi, Mahyar and Torres-Rua, Alfonso F. and Kustas, William P. and Nieto, H{\'{e}}ctor and Coopmans, Calvin and McKee, Mac},
  doi       = {10.1007/s00271-018-0613-9},
  issn      = {14321319},
  journal   = {Irrigation Science},
  number    = {3},
  pages     = {407--429},
  publisher = {Springer Berlin Heidelberg},
  title     = {{Assessment of different methods for shadow detection in high-resolution optical imagery and evaluation of shadow impact on calculation of NDVI, and evapotranspiration}},
  volume    = {37},
  year      = {2019}
}

@article{Amgalan2020,
  abstract      = {Physical or geographic location proves to be an important feature in many data science models, because many diverse natural and social phenomenon have a spatial component. Spatial autocorrelation measures the extent to which locally adjacent observations of the same phenomenon are correlated. Although statistics like Moran's {\$}I{\$} and Geary's {\$}C{\$} are widely used to measure spatial autocorrelation, they are slow: all popular methods run in {\$}\backslashOmega(n{\^{}}2){\$} time, rendering them unusable for large data sets, or long time-courses with moderate numbers of points. We propose a new {\$}S{\_}A{\$} statistic based on the notion that the variance observed when merging pairs of nearby clusters should increase slowly for spatially autocorrelated variables. We give a linear-time algorithm to calculate {\$}S{\_}A{\$} for a variable with an input agglomeration order (available at github.com/aamgalan/spatial{\_}autocorrelation). For a typical dataset of {\$}n \backslashapprox 63,000{\$} points, our {\$}S{\_}A{\$} autocorrelation measure can be computed in 1 second, versus 2 hours or more for Moran's {\$}I{\$} and Geary's {\$}C{\$}. Through simulation studies, we demonstrate that {\$}S{\_}A{\$} identifies spatial correlations in variables generated with spatially-dependent model half an order of magnitude earlier than either Moran's {\$}I{\$} or Geary's {\$}C{\$}. Finally, we prove several theoretical properties of {\$}S{\_}A{\$}: namely that it behaves as a true correlation statistic, and is invariant under addition or multiplication by a constant.},
  archiveprefix = {arXiv},
  arxivid       = {2010.08676},
  author        = {Amgalan, Anar and Mujica-Parodi, Lilianne R. and Skiena, Steven S.},
  doi           = {10.2307/2529248},
  eprint        = {2010.08676},
  issn          = {0006341X},
  journal       = {Biometrics},
  number        = {4},
  pages         = {729},
  title         = {{Fast Spatial Autocorrelation}},
  volume        = {30},
  year          = {2020}
}

@article{Arslan2014,
  abstract = {In this study, we examined the ability of reflectance spectroscopy to predict some of the most important soil parameters for irrigation such as field capacity (FC), wilting point (WP), clay, sand, and silt content. FC and WP were determined for 305 soil samples. In addition to these soil analyses, clay, silt, and sand contents of 145 soil samples were detected. Raw spectral reflectance (raw) of these soil samples, between 350 and 2,500-nm wavelengths, was measured. In addition, first order derivatives of the reflectance (first) were calculated. Two different statistical approaches were used in detecting soil properties from hyperspectral data. Models were evaluated using the correlation of coefficient (r), coefficient of determination (R (2)), root mean square error (RMSE), and residual prediction deviation (RPD). In the first method, two appropriate wavelengths were selected for raw reflectance and first derivative separately for each soil property. Selection of wavelengths was carried out based on the highest positive and negative correlations between soil property and raw reflectance or first order derivatives. By means of detected wavelengths, new combinations for each soil property were calculated using rationing, differencing, normalized differencing, and multiple regression techniques. Of these techniques, multiple regression provided the best correlation (P {\textless} 0.01) for selected wavelengths and all soil properties. To estimate FC, WP, clay, sand, and silt, multiple regression equations based on first(2,310)-first(2,360), first(2,310)-first(2,360), first(2,240)-first(1,320), first(2,240)-first(1,330), and raw(2,260)-raw(360) were used. Partial least square regression (PLSR) was performed as the second method. Raw reflectance was a better predictor of WP and FC, whereas first order derivative was a better predictor of clay, sand, and silt content. According to RPD values, statistically excellent predictions were obtained for FC (2.18), and estimations for WP (2.0), clay (1.8), and silt (1.63) were acceptable. However, sand values were poorly predicted (RDP = 0.63). In conclusion, both of the methods examined here offer quick and inexpensive means of predicting soil properties using spectral reflectance data.},
  author   = {Arslan, Hakan and Tasan, Mehmet and Yildirim, Demet and Koksal, Ey{\"{u}}p Selim and Cemek, Bilal},
  doi      = {10.1007/s10661-014-3761-2},
  issn     = {15732959},
  journal  = {Environmental Monitoring and Assessment},
  keywords = {Field capacity,First order derivatives,Soil texture,Spectral reflectance,Wilting point},
  number   = {8},
  pages    = {5077--5088},
  pmid     = {24715616},
  title    = {{Predicting field capacity, wilting point, and the other physical properties of soils using hyperspectral reflectance spectroscopy: Two different statistical approaches}},
  volume   = {186},
  year     = {2014}
}

@article{Ball2017,
  abstract      = {In recent years, deep learning (DL), a re-branding of neural networks (NNs), has risen to the top in numerous areas, namely computer vision (CV), speech recognition, natural language processing, etc. Whereas remote sensing (RS) possesses a number of unique challenges, primarily related to sensors and applications, inevitably RS draws from many of the same theories as CV; e.g., statistics, fusion, and machine learning, to name a few. This means that the RS community should be aware of, if not at the leading edge of, of advancements like DL. Herein, we provide the most comprehensive survey of state-of-the-art RS DL research. We also review recent new developments in the DL field that can be used in DL for RS. Namely, we focus on theories, tools and challenges for the RS community. Specifically, we focus on unsolved challenges and opportunities as it relates to (i) inadequate data sets, (ii) human understandable solutions for modelling physical phenomena, (iii) Big Data, (iv) non-traditional heterogeneous data sources, (v) DL architectures and learning algorithms for spectral, spatial and temporal data, (vi) transfer learning, (vii) an improved theoretical understanding of DL systems, (viii) high barriers to entry, and (ix) training and optimizing the DL.},
  archiveprefix = {arXiv},
  arxivid       = {1709.00308},
  author        = {Ball, John E. and Anderson, Derek T. and Chan, Chee Seng},
  doi           = {10.1117/1.jrs.11.042609},
  eprint        = {1709.00308},
  issn          = {23318422},
  journal       = {arXiv},
  keywords      = {Big data,Computer vision,Deep learning,Hyperspectral,Multispectral,Remote sensing},
  number        = {4},
  title         = {{A comprehensive survey of deep learning in remote sensing: Theories, tools and challenges for the community}},
  volume        = {11},
  year          = {2017}
}

@article{Xie2020,
  abstract  = {The current methods of phenotyping for breeding lines require a lot of time, labor and cost. In recent years, unmanned aerial system (UAS) has paved the way for the development of field high-throughput phenotyping for crops rapidly. Different sensors such as regular RGB camera (Red, Green and Blue), multispectral imaging camera (several wavebands), hyperspectral imaging camera (hundreds and even thousands of wavebands), thermal imaging sensor and light detection and ranging (LiDAR) sensor can be placed on unmanned aerial vehicle (UAV) to collect remote sensing data in field-scale trials. Based on this technique, the plant traits (e.g., yield, biomass, height, and leaf area index) can be estimated non-destructively, which is critical for high-throughput phenotyping in agriculture. Compared with vehicle-based ground sensors, UAS can increase throughput and frequency for phenotyping. It is low-cost and could provide high-resolution images compared with satellite-based technique. Based upon the phenotypic traits, those crops with high yield and strong stress resistance (e.g., disease resistance and salt resistance) can be selected, which could finally improve the production. This paper talked about the plant high-throughput phenotyping traits based on the sensors on the UAV. Also, the challenges and obstacles of UAV (e.g., flight safety, flight altitude, flight time, and sensor accuracy) were analyzed. In order to provide the updated information of the relationships between remote sensing information taken from UAV and plant phenotyping traits, we summarized the sensors, plants and traits reported in previous research articles. As a result, the review can be very useful for researchers to use appropriate UAV-based sensors to carry out plant phenotyping experiments, and for farmers to use this advanced technology in managing agricultural production.},
  author    = {Xie, Chuanqi and Yang, Ce},
  doi       = {10.1016/j.compag.2020.105731},
  issn      = {01681699},
  journal   = {Computers and Electronics in Agriculture},
  keywords  = {Phenotyping,Plant,Sensors,Traits,Unmanned aerial system (UAS)},
  number    = {October 2019},
  pages     = {105731},
  publisher = {Elsevier},
  title     = {{A review on plant high-throughput phenotyping traits using UAV-based sensors}},
  volume    = {178},
  year      = {2020}
}

@article{Chivasa2017,
  abstract  = {Maize (Zea mays L.) is the second most commonly grown crop worldwide and number one staple food in Africa where it accounts for more than 50{\%} of the energy requirements. However, despite its widespread cultivation and the significance of maize information in Africa, maize crop maps and yield forecasts are hardly available. Yet, systematic area, spatial distribution, and maize yield estimates are important in understanding and addressing food security in Africa. Objective monitoring of maize yield statisics in a systematic way is possible with remotely sensed data. However, absence of maize yield forecasts using remote sensing in Africa has been attributed to the cost of acquiring satellite imagery and the heterogeneity of agricultural landscapes. The recent advances in sensors technology and availability of free high-resolution (spatial and temporal) multispectral satellite images afford an opportunity to forecast maize yield as well as mapping its spatial distribution in near real-time basis. This review gives an overview of maize yield estimation using remotely sensed information and its potential application in a fragmented and highly granular agricultural landscapes in Africa, including inherent challenges and research needs. The review was motivated by challenges faced by researchers and national agricultural statistical services agents when forecasting maize yield using conventional ground-based survey methods. These problems include, but are not limited to, restricted accuracy, and cost and time spent resulting in missed opportunities in food security early warning systems and proper developmental interventions. We conclude that by picking multispectral sensors with high spatial, temporal, and spectral resolution, as well as appropriate classification techniques and accurate ground-truthing data, remote sensing can be a practical option for estimating maize grain yield and its spatio-temporal dynamics in heterogeneous African agricultural landscapes for designing appropriate developmental interventions and technological out scaling.},
  author    = {Chivasa, Walter and Mutanga, Onisimo and Biradar, Chandrashekhar},
  doi       = {10.1080/01431161.2017.1365390},
  issn      = {13665901},
  journal   = {International Journal of Remote Sensing},
  number    = {23},
  pages     = {6816--6845},
  publisher = {Taylor {\&} Francis},
  title     = {{Application of remote sensing in estimating maize grain yield in heterogeneous african agricultural landscapes: A review}},
  volume    = {38},
  year      = {2017}
}

@article{Gomez2019,
  abstract = {Forest ecosystems provide a host of services and societal benefits, including carbon storage, habitat for fauna, recreation, and provision of wood or non-wood products. In a context of complex demands on forest resources, identifying priorities for biodiversity and carbon budgets require accurate tools with sufficient temporal frequency. Moreover, understanding long term forest dynamics is necessary for sustainable planning and management. Remote sensing (RS) is a powerful means for analysis, synthesis, and report, providing insights and contributing to inform decisions upon forest ecosystems. In this communication we review current applications of RS techniques in Spanish forests, examining possible trends, needs, and opportunities offered by RS in a forestry context. Currently, wall-to-wall optical and LiDAR data are extensively used for a wide range of applications—many times in combination—whilst radar or hyperspectral data are rarely used in the analysis of Spanish forests. Unmanned Aerial Vehicles (UAVs) carrying visible and infrared sensors are gaining ground in acquisition of data locally and at small scale, particularly for health assessments. Forest fire identification and characterization are prevalent applications at the landscape scale, whereas structural assessments are the most widespread analyses carried out at limited extents. Unparalleled opportunities are offered by the availability of diverse RS data like those provided by the European Copernicus programme and recent satellite LiDAR launches, processing capacity, and synergies with other ancillary sources to produce information of our forests. Overall, we live in times of unprecedented opportunities for monitoring forest ecosystems with a growing support from RS technologies.},
  author   = {G{\'{o}}mez, Cristina and Alejandro, Pablo and Hermosilla, Txomin and Montes, Fernando and Pascual, Cristina and Ruiz, Luis {\'{A}}ngel and {\'{A}}lvarez-Taboada, Flor and Tanase, Mihai A. and Valbuena, Rub{\'{e}}n},
  doi      = {10.5424/fs/2019281-14221},
  issn     = {21719845},
  journal  = {Forest Systems},
  keywords = {Forest fire,Forest health,Forest structure,LiDAR,Optical,Radar,UAV},
  number   = {1},
  pages    = {1--33},
  title    = {{Remote sensing for the Spanish forests in the 21stcentury: A review of advances, needs, and opportunities}},
  volume   = {28},
  year     = {2019}
}

@online{SatelliteMissionsDirectory,
  note    = {\\\url{https://directory.eoportal.org/web/eoportal/satellite-missions}},
  title   = {{Satellite Missions Directory}},
  urldate = {2021-03-02}
}

@online{Gaofen1,
  note    = {\url{https://directory.eoportal.org/web/eoportal/satellite-missions/g/gaofen-1}},
  title   = {{Gaofen-1}},
  urldate = {2021-03-02}
}

@online{Gaofen2,
  note    = {\url{https://directory.eoportal.org/web/eoportal/satellite-missions/g/gaofen-2}},
  title   = {{Gaofen-2}},
  urldate = {2021-03-02}
}

@online{PlanetScope,
  note    = {\url{https://earth.esa.int/eogateway/missions/planetscope}},
  title   = {{PlanetScope}},
  urldate = {2021-03-02}
}

@online{Sentinel2,
  note    = {\url{https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi}},
  title   = {{Sentinel-2 MSI}},
  urldate = {2021-03-02}
}

@online{Landsat8,
  note    = {\url{https://www.usgs.gov/core-science-systems/nli/landsat/landsat-8}},
  title   = {{Landsat 8}},
  urldate = {2021-03-02}
}

@online{Landsat7,
  note    = {\url{https://www.usgs.gov/core-science-systems/nli/landsat/landsat-7}},
  title   = {{Landsat 7}},
  urldate = {2021-03-02}
}

@online{WorldView2,
  note    = {\\\url{https://directory.eoportal.org/web/eoportal/satellite-missions/v-w-x-y-z/worldview-2}},
  title   = {{WorldView-2}},
  urldate = {2021-03-02}
}

@online{WorldView3,
  note    = {\\\url{https://directory.eoportal.org/web/eoportal/satellite-missions/v-w-x-y-z/worldview-3}},
  title   = {{WorldView-3}},
  urldate = {2021-03-02}
}

@article{Karthikeyan2020,
  abstract  = {The global population is expected to reach 9.8 billion by 2050. There is an exponential growth of food production to meet the needs of the growing population. However, the limited land and water resources, climate change, and an increase in extreme events likely to pose a significant threat for achieving the sustainable agriculture goal. Given these challenges, food security is included in the United Nations' Sustainable Development Goals (SDGs). Since the advent of Sputnik, followed by the Explorer missions, satellite remote sensing is assisting us in collecting the data at global scales. In this work, we review how satellite remote sensing information is utilized to assess and manage agriculture, an important component of ecohydrology. Overall, three critical aspects of agriculture are considered: (a) crop growth and yield through empirical models, physics-based models, and data assimilation in crop models, (b) applications pertaining to irrigation, which include mapping irrigation areas and quantification of irrigation, and (c) crop losses due to pests, diseases, crop lodging, and weeds. The emphasis is on satellite sensors in optical, thermal, microwave, and fluorescence frequencies. We conclude the review with an outlook of challenges and recommendations. This paper is the first of a two-part review series. The second part reviews the role of satellite remote sensing in water security, wherein we discuss the aspects of water quality and quantity along with extremes (floods and droughts).},
  author    = {Karthikeyan, L. and Chawla, Ila and Mishra, Ashok K.},
  doi       = {10.1016/j.jhydrol.2020.124905},
  issn      = {00221694},
  journal   = {Journal of Hydrology},
  keywords  = {Agriculture,Crop losses,Crop yield,Data assimilation,Irrigation,Remote sensing},
  number    = {March},
  pages     = {124905},
  publisher = {Elsevier},
  title     = {{A review of remote sensing applications in agriculture for food security: Crop growth and yield, irrigation, and crop losses}},
  volume    = {586},
  year      = {2020}
}

@article{Maimaitijiang2020,
  abstract = {Non-destructive crop monitoring over large areas with high efficiency is of great significance in precision agriculture and plant phenotyping, as well as decision making with regards to grain policy and food security. The goal of this research was to assess the potential of combining canopy spectral information with canopy structure features for crop monitoring using satellite/unmanned aerial vehicle (UAV) data fusion and machine learning. Worldview-2/3 satellite data were tasked synchronized with high-resolution RGB image collection using an inexpensive unmanned aerial vehicle (UAV) at a heterogeneous soybean (Glycine max (L.) Merr.) field. Canopy spectral information (i.e., vegetation indices) was extracted fromWorldview-2/3 data, and canopy structure information (i.e., canopy height and canopy cover) was derived from UAV RGB imagery. Canopy spectral and structure information and their combination were used to predict soybean leaf area index (LAI), aboveground biomass (AGB), and leaf nitrogen concentration (N) using partial least squares regression (PLSR), random forest regression (RFR), support vector regression (SVR), and extreme learning regression (ELR) with a newly proposed activation function. The results revealed that: (1) UAV imagery-derived high-resolution and detailed canopy structure features, canopy height, and canopy coverage were significant indicators for crop growth monitoring, (2) integration of satellite imagery-based rich canopy spectral information with UAV-derived canopy structural features using machine learning improved soybean AGB, LAI, and leaf N estimation on using satellite or UAV data alone, (3) adding canopy structure information to spectral features reduced background soil effect and asymptotic saturation issue to some extent and led to better model performance, (4) the ELR model with the newly proposed activated function slightly outperformed PLSR, RFR, and SVR in the prediction of AGB and LAI, while RFR provided the best result for N estimation. This study introduced opportunities and limitations of satellite/UAV data fusion using machine learning in the context of crop monitoring.},
  author   = {Maimaitijiang, Maitiniyazi and Sagan, Vasit and Sidike, Paheding and Daloye, Ahmad M. and Erkbol, Hasanjan and Fritschi, Felix B.},
  doi      = {10.3390/RS12091357},
  issn     = {20724292},
  journal  = {Remote Sensing},
  keywords = {Activation function,Crop monitoring,Data fusion,Extreme learning machine (ELM),Machine learning,Unmanned aerial vehicle (UAV)},
  number   = {9},
  title    = {{Crop monitoring using satellite/UAV data fusion and machine learning}},
  volume   = {12},
  year     = {2020}
}

@article{Wolanin2020,
  abstract  = {Forecasting crop yields is becoming increasingly important under the current context in which food security needs to be ensured despite the challenges brought by climate change, an expanding world population accompanied by rising incomes, increasing soil erosion, and decreasing water resources. Temperature, radiation, water availability and other environmental conditions influence crop growth, development, and final grain yield in a complex nonlinear manner. Machine learning (ML) techniques, and deep learning (DL) methods in particular, can account for such nonlinear relations between yield and its covariates. However, they typically lack transparency and interpretability, since the way the predictions are derived is not directly evident. Yet, in the context of yield forecasting, understanding which are the underlying factors behind both a predicted loss or gain is of great relevance. Here, we explore how to benefit from the increased predictive performance of DL methods while maintaining the ability to interpret how the models achieve their results. To do so, we applied a deep neural network to multivariate time series of vegetation and meteorological data to estimate the wheat yield in the Indian Wheat Belt. Then, we visualized and analyzed the features and yield drivers learned by the model with the use of regression activation maps. The DL model outperformed other tested models (ridge regression and random forest) and facilitated the interpretation of variables and processes that lead to yield variability. The learned features were mostly related to the length of the growing season, and temperature and light conditions during this time. For example, our results showed that high yields in 2012 were associated with low temperatures accompanied by sunny conditions during the growing period. The proposed methodology can be used for other crops and regions in order to facilitate application of DL models in agriculture.},
  author    = {Wolanin, Aleksandra and Mateo-Garc{\'{i}}a, Gonzalo and Camps-Valls, Gustau and G{\'{o}}mez-Chova, Luis and Meroni, Michele and Duveiller, Gregory and Liangzhi, You and Guanter, Luis},
  doi       = {10.1088/1748-9326/ab68ac},
  issn      = {1748-9326},
  journal   = {Environmental Research Letters},
  keywords  = {Indian Wheat Belt,deep learning (DL),explainable artificial intelligence (XAI),food security,regression activation map (RAM),remote sensing,wheat yield},
  number    = {2},
  pages     = {024019},
  publisher = {IOP Publishing},
  title     = {{Estimating and understanding crop yields with explainable deep learning in the Indian Wheat Belt}},
  volume    = {15},
  year      = {2020}
}

@article{Salmivaara2020,
  abstract = {Forest harvesting operations with heavy machinery can lead to significant soil rutting. Risks of rutting depend on the soil bearing capacity which has considerable spatial and temporal variability. Trafficability prediction is required in the selection of suitable operation sites for a given time window and conditions, and for on-site route optimization during the operation. Integrative tools are necessary to plan and carry out forest operations with minimal negative ecological and economic impacts. This study demonstrates a trafficability prediction framework that utilizes a spatial hydrological model and a wide range of spatial data. Trafficability was approached by producing a rut depth prediction map at a 16 × 16 m grid resolution, based on the outputs of a general linear mixed model developed using field data from Southern Finland, modelled daily soil moisture, spatial forest inventory and topography data, along with field measured rolling resistance and information on the mass transported through the grid cells. Dynamic rut depth prediction maps were produced by accounting for changing weather conditions through hydrological modelling. We also demonstrated a generalization of the rolling resistance coefficient, measured with harvester CAN-bus channel data. Future steps towards a nationwide prediction framework based on continuous data flow, process-based modelling and machine learning are discussed.},
  author   = {Salmivaara, Aura and Launiainen, Samuli and Perttunen, Jari and Nevalainen, Paavo and Pohjankukka, Jonne and Ala-Ilom{\"{a}}ki, Jari and Sir{\'{e}}n, Matti and Laur{\'{e}}n, Ari and Tuominen, Sakari and Uusitalo, Jori and Pahikkala, Tapio and Heikkonen, Jukka and Fin{\'{e}}r, Leena},
  doi      = {10.1093/forestry/cpaa010},
  issn     = {0015-752X},
  journal  = {Forestry: An International Journal of Forest Research},
  number   = {5},
  pages    = {662--674},
  title    = {{Towards dynamic forest trafficability prediction using open spatial data, hydrological modelling and sensor technology}},
  volume   = {93},
  year     = {2020}
}

@online{GoogleEarthEngine,
  note    = {\url{https://developers.google.com/earth-engine/}},
  title   = {{Google Earth Engine}},
  urldate = {2021-03-05}
}

@article{Koirala2019,
  abstract  = {A review of developments in the rapidly developing field of deep learning is presented. Recommendations are made for original contributions to the literature, as opposed to formulaic applications of established methods to new application areas (e.g., to new crops), including the use of standard metrics (e.g., F1 score, the harmonic mean between Precision and Recall) for model comparison involving binary classification. A recommendation for the provision and use of publically available fruit-in-orchard image sets is made, to allow method comparisons and for implementation of transfer learning for deep learning models trained on the large public generic datasets. Emphasis is placed on practical aspects for application of deep learning models for the task of fruit detection and localisation, in support of tree crop load estimation. Approaches to the extrapolation of tree image counts to orchard yield estimation are also reviewed, dealing with the issue of occluded fruit in imaging. The review is intended to assist new users of deep learning image processing techniques, and to influence the direction of the coming body of application work on fruit detection.},
  author    = {Koirala, Anand and Walsh, Kerry B. and Wang, Zhenglin and McCarthy, Cheryl},
  doi       = {10.1016/j.compag.2019.04.017},
  issn      = {01681699},
  journal   = {Computers and Electronics in Agriculture},
  keywords  = {Convolutional neural network,Machine vision,Precision horticulture,Tree crop,YOLO,Yield estimation},
  number    = {January},
  pages     = {219--234},
  publisher = {Elsevier},
  title     = {{Deep learning – Method overview and review of use for fruit detection and yield estimation}},
  volume    = {162},
  year      = {2019}
}

@article{Archontoulis2020,
  abstract = {We used the Agricultural Production Systems sIMulator (APSIM) to predict and explain maize and soybean yields, phenology, and soil water and nitrogen (N) dynamics during the growing season in Iowa, USA. Historical, current and forecasted weather data were used to drive simulations, which were released in public four weeks after planting. In this paper, we (1) describe the methodology used to perform forecasts; (2) evaluate model prediction accuracy against data collected from 10 locations over four years; and (3) identify inputs that are key in forecasting yields and soil N dynamics. We found that the predicted median yield at planting was a very good indicator of end-of-season yields (relative root mean square error [RRMSE] of ∼20{\%}). For reference, the prediction at maturity, when all the weather was known, had a RRMSE of 14{\%}. The good prediction at planting time was explained by the existence of shallow water tables, which decreased model sensitivity to unknown summer precipitation by 50–64{\%}. Model initial conditions and management information accounted for one-fourth of the variation in maize yield. End of season model evaluations indicated that the model simulated well crop phenology (R2= 0.88), root depth (R2= 0.83), biomass production (R2= 0.93), grain yield (R2= 0.90), plant N uptake (R2= 0.87), soil moisture (R2= 0.42), soil temperature (R2= 0.93), soil nitrate (R2= 0.77), and water table depth (R2= 0.41). We concluded that model set-up by the user (e.g. inclusion of water table), initial conditions, and early season measurements are very important for accurate predictions of soil water, N and crop yields in this environment.},
  author   = {Archontoulis, Sotirios V. and Castellano, Michael J. and Licht, Mark A. and Nichols, Virginia and Baum, Mitch and Huber, Isaiah and Martinez-Feria, Rafael and Puntel, Laila and Ord{\'{o}}{\~{n}}ez, Raziel A. and Iqbal, Javed and Wright, Emily E. and Dietzel, Ranae N. and Helmers, Matt and Vanloocke, Andy and Liebman, Matt and Hatfield, Jerry L. and Herzmann, Daryl and C{\'{o}}rdova, S. Carolina and Edmonds, Patrick and Togliatti, Kaitlin and Kessler, Ashlyn and Danalatos, Gerasimos and Pasley, Heather and Pederson, Carl and Lamkey, Kendall R.},
  doi      = {10.1002/csc2.20039},
  issn     = {14350653},
  journal  = {Crop Science},
  number   = {2},
  pages    = {721--738},
  title    = {{Predicting crop yields and soil-plant nitrogen dynamics in the US Corn Belt}},
  volume   = {60},
  year     = {2020}
}

@article{He2016,
  abstract      = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  arxivid       = {1512.03385},
  author        = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  doi           = {10.1109/CVPR.2016.90},
  eprint        = {1512.03385},
  issn          = {10636919},
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  pages         = {770--778},
  title         = {{Deep residual learning for image recognition}},
  volume        = {2016-December},
  year          = {2016}
}

@article{Roosjen2020,
  abstract = {BACKGROUND: The fruit fly Drosophila suzukii, or spotted wing drosophila (SWD), is a serious pest worldwide, attacking many soft-skinned fruits. An efficient monitoring system that identifies and counts SWD in crops and their surroundings is therefore essential for integrated pest management (IPM) strategies. Existing methods, such as catching flies in liquid bait traps and counting them manually, are costly, time-consuming and labour-intensive. To overcome these limitations, we studied insect trap monitoring using image-based object detection with deep learning. RESULTS: Based on an image database with 4753 annotated SWD flies, we trained a ResNet-18-based deep convolutional neural network to detect and count SWD, including sex prediction and discrimination. The results show that SWD can be detected with an area under the precision recall curve (AUC) of 0.506 (female) and 0.603 (male) in digital images taken from a static position. For images collected using an unmanned aerial vehicle (UAV), the algorithm detected SWD individuals with an AUC of 0.086 (female) and 0.284 (male). The lower AUC for the aerial imagery was due to lower image quality caused by stabilisation manoeuvres of the UAV during image collection. CONCLUSION: Our results indicate that it is possible to monitor SWD using deep learning and object detection. Moreover, the results demonstrate the potential of UAVs to monitor insect traps, which could be valuable in the development of autonomous insect monitoring systems and IPM. {\textcopyright} 2020 The Authors. Pest Management Science published by John Wiley {\&} Sons Ltd on behalf of Society of Chemical Industry.},
  author   = {Roosjen, Peter P.J. and Kellenberger, Benjamin and Kooistra, Lammert and Green, David R. and Fahrentrapp, Johannes},
  doi      = {10.1002/ps.5845},
  issn     = {15264998},
  journal  = {Pest Management Science},
  keywords = {Drosophila suzukii,deep learning,integrated pest management (IPM),object detection,unmanned aerial vehicle (UAV)},
  number   = {9},
  pages    = {2994--3002},
  pmid     = {32246738},
  title    = {{Deep learning for automated detection of Drosophila suzukii: potential for UAV-based monitoring}},
  volume   = {76},
  year     = {2020}
}

@article{Ren2017,
  abstract      = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
  archiveprefix = {arXiv},
  arxivid       = {1506.01497},
  author        = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  doi           = {10.1109/TPAMI.2016.2577031},
  eprint        = {1506.01497},
  issn          = {01628828},
  journal       = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords      = {Object detection,convolutional neural network,region proposal},
  number        = {6},
  pages         = {1137--1149},
  pmid          = {27295650},
  title         = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
  volume        = {39},
  year          = {2017}
}

@incollection{Liu2016,
  abstract  = {It has been shown that the template based approaches could quickly estimate 6D pose of texture-less objects from a monocular image. However, they tend to be slow when the number of templates amounts to tens of thousands for handling a wider range of 3D object pose. To alleviate this problem, we propose a novel image feature and a tree-structured model. Our proposed perspectively cumulated orientation feature (PCOF) is based on the orientation histograms extracted from randomly generated 2D projection images using 3D CAD data, and the template using PCOF explicitly handle a certain range of 3D object pose. The hierarchical pose trees (HPT) is built by clustering 3D object pose and reducing the resolutions of templates, and HPT accelerates 6D pose estimation based on a coarse-to-fine strategy with an image pyramid. In the experimental evaluation on our texture-less object dataset, the combination of PCOF and HPT showed higher accuracy and faster speed in comparison with state-of-the-art techniques.},
  address   = {Cham},
  author    = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  booktitle = {Eccv},
  doi       = {10.1007/978-3-319-46448-0_2},
  editor    = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
  pages     = {21--37},
  pmid      = {10463930},
  publisher = {Springer International Publishing},
  series    = {Lecture Notes in Computer Science},
  title     = {{SSD: Single Shot MultiBox Detector}},
  volume    = {9905},
  year      = {2016}
}

@article{Sandler2018,
  abstract      = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters.},
  archiveprefix = {arXiv},
  arxivid       = {1801.04381},
  author        = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang Chieh},
  doi           = {10.1109/CVPR.2018.00474},
  eprint        = {1801.04381},
  issn          = {10636919},
  journal       = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  pages         = {4510--4520},
  publisher     = {IEEE},
  title         = {{MobileNetV2: Inverted Residuals and Linear Bottlenecks}},
  year          = {2018}
}

@article{Kang2020,
  abstract = {Crop yield estimates over large areas are conventionally made using weather observations, but a comprehensive understanding of the effects of various environmental indicators, observation frequency, and the choice of prediction algorithm remains elusive. Here we present a thorough assessment of county-level maize yield prediction in U.S. Midwest using six statistical/machine learning algorithms (Lasso, Support Vector Regressor, Random Forest, XGBoost, Long-short term memory (LSTM), and Convolutional Neural Network (CNN)) and an extensive set of environmental variables derived from satellite observations, weather data, land surface model results, soil maps, and crop progress reports. Results show that seasonal crop yield forecasting benefits from both more advanced algorithms and a large composite of information associated with crop canopy, environmental stress, phenology, and soil properties (i.e. hundreds of features). The XGBoost algorithm outperforms other algorithms both in accuracy and stability, while deep neural networks such as LSTM and CNN are not advantageous. The compositing interval (8-day, 16-day or monthly) of time series variable does not have significant effects on the prediction. Combining the best algorithm and inputs improves the prediction accuracy by 5{\%} when compared to a baseline statistical model (Lasso) using only basic climatic and satellite observations. Reasonable county-level yield foresting is achievable from early June, almost four months prior to harvest. At the national level, early-season (June and July) prediction from the best model outperforms that of the United States Department of Agriculture (USDA) World Agricultural Supply and Demand Estimates (WASDE). This study provides insights into practical crop yield forecasting and the understanding of yield response to climatic and environmental conditions.},
  author   = {Kang, Yanghui and Ozdogan, Mutlu and Zhu, Xiaojin and Ye, Zhiwei and Hain, Christopher and Anderson, Martha},
  doi      = {10.1088/1748-9326/ab7df9},
  issn     = {17489326},
  journal  = {Environmental Research Letters},
  keywords = {climate impact,crop yields,data-driven,deep learning,machine learning},
  number   = {6},
  title    = {{Comparative assessment of environmental variables and machine learning algorithms for maize yield prediction in the US Midwest}},
  volume   = {15},
  year     = {2020}
}

@article{Chen2016,
  abstract      = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  archiveprefix = {arXiv},
  arxivid       = {1603.02754},
  author        = {Chen, Tianqi and Guestrin, Carlos},
  doi           = {10.1145/2939672.2939785},
  eprint        = {1603.02754},
  issn          = {0146-4833},
  journal       = {arXiv},
  keywords      = {large-scale machine learning},
  pages         = {1--6},
  pmid          = {22942019},
  title         = {{XGBoost: A Scalable Tree Boosting System}},
  year          = {2016}
}

@article{Lee2019,
  abstract  = {This paper proposes a self-predictable crop yield platform (SCYP) based on crop diseases using deep learning that collects weather information (temperature, humidity, sunshine, precipitation, etc.) and farm status information (harvest date, disease information, crop status, ground temperature, etc.), diagnoses crop diseases by using convolutional neural network (CNN), and predicts crop yield based on factors such as climate change, crop diseases, and others by using artificial neural network (ANN). The SCYP consists of an image preprocessing module (IPM) to determine crop diseases through the Google Vision API and image resizing, a crop disease diagnosis module (CDDM) based on CNN to diagnose the types and extent of crop diseases through photographs, and a crop yield prediction module (CYPM) based on ANN by using information of crop diseases, remaining time until harvest (based on the date), current temperature, humidity and precipitation (amount of snowfall) in the area, sunshine amount, ground temperature, atmospheric pressure, moisture evaporation in the ground, etc. Four experiments were conducted to verify the efficiency of the SCYP. In the CDMM, the accuracy and operation time of each model were measured using three neural network models: CNN, region-CNN(R-CNN), and you only look once (YOLO). In the CYPM, rectified linear unit (ReLU), Sigmoid, and Step activation functions were compared to measure ANN accuracy. The accuracy of CNN was about 3.5{\%} higher than that of R-CNN and about 5.4{\%} higher than that of YOLO. The operation time of CNN was about 37 s less than that of R-CNN and about 72 s less than that of YOLO. The CDDM had slightly less operation time, but in this paper, we prefer accuracy over operation time to diagnose crop diseases efficiently and accurately. When the activation function of the ANN used in the CYPM was ReLU, the accuracy of the ANN was 2{\%} higher than that of Sigmoid and 7{\%} higher than that of Step. The CYPM prediction was about 34{\%} more accurate when using multiple diseases than when not using them. Therefore, the SCYP can predict farm yields more accurately than traditional methods.},
  author    = {Lee, SangSik and Jeong, YiNa and Son, SuRak and Lee, ByungKwan},
  doi       = {10.3390/su11133637},
  issn      = {2071-1050},
  journal   = {Sustainability},
  keywords  = {ANN,CNN,Crop disease diagnosis,Image preprocessing,Yield prediction},
  number    = {13},
  pages     = {3637},
  publisher = {MDPI AG},
  title     = {{A Self-Predictable Crop Yield Platform (SCYP) Based On Crop Diseases Using Deep Learning}},
  volume    = {11},
  year      = {2019}
}

@article{Jiang2020,
  abstract = {Understanding large-scale crop growth and its responses to climate change are critical for yield estimation and prediction, especially under the increased frequency of extreme climate and weather events. County-level corn phenology varies spatially and interannually across the Corn Belt in the United States, where precipitation and heat stress presents a temporal pattern among growth phases (GPs) and vary interannually. In this study, we developed a long short-term memory (LSTM) model that integrates heterogeneous crop phenology, meteorology, and remote sensing data to estimate county-level corn yields. By conflating heterogeneous phenology-based remote sensing and meteorological indices, the LSTM model accounted for 76{\%} of yield variations across the Corn Belt, improved from 39{\%} of yield variations explained by phenology-based meteorological indices alone. The LSTM model outperformed least absolute shrinkage and selection operator (LASSO) regression and random forest (RF) approaches for end-of-the-season yield estimation, as a result of its recurrent neural network structure that can incorporate cumulative and nonlinear relationships between corn yield and environmental factors. The results showed that the period from silking to dough was most critical for crop yield estimation. The LSTM model presented a robust yield estimation under extreme weather events in 2012, which reduced the root-mean-square error to 1.47 Mg/ha from 1.93 Mg/ha for LASSO and 2.43 Mg/ha for RF. The LSTM model has the capability to learn general patterns from high-dimensional (spectral, spatial, and temporal) input features to achieve a robust county-level crop yield estimation. This deep learning approach holds great promise for better understanding the global condition of crop growth based on publicly available remote sensing and meteorological data.},
  author   = {Jiang, Hao and Hu, Hao and Zhong, Renhai and Xu, Jinfan and Xu, Jialu and Huang, Jingfeng and Wang, Shaowen and Ying, Yibin and Lin, Tao},
  doi      = {10.1111/gcb.14885},
  issn     = {13652486},
  journal  = {Global Change Biology},
  keywords = {climate change impact,corn yield,deep learning,geospatial discovery,phenology},
  number   = {3},
  pages    = {1754--1766},
  pmid     = {31789455},
  title    = {{A deep learning approach to conflating heterogeneous geospatial data for corn yield estimation: A case study of the US Corn Belt at the county level}},
  volume   = {26},
  year     = {2020}
}

@article{Lin2020,
  abstract  = {Large-scale crop yield estimation is critical for understanding the dynamics of global food security. Understanding and quantifying the temporal cumulative effect of crop growth and spatial variances across different regions remains challenging for large-scale crop yield estimation. In this study, a deep spatial-temporal learning framework, named DeepCropNet (DCN), has been developed to hierarchically capture the features for county-level corn yield estimation. The temporal features are learned by an attention-based long short-term memory network and the spatial features are learned by the multi-task learning (MTL) output layers. The DCN model has been applied to quantify the relationship between meteorological factors and the county-level corn yield in the US Corn Belt from 1981 to 2016. Three meteorological factors, including growing degree days, killing degree days, and precipitation, are used as time-series inputs. The results show that DCN provides an improved estimation accuracy (RMSE = 0.82 Mg ha-1) as compared to that of conventional methods such as LASSO (RMSE = 1.14 Mg ha-1) and Random Forest (RMSE = 1.05 Mg ha-1). Temporally, the attention values computed from the temporal learning module indicate that DCN captures the temporal cumulative effect and this temporal pattern is consistent across all states. Spatially, the spatial learning module improves the estimation accuracy based on the regional specific features captured by the MTL mechanism. The study highlights that the DCN model provides a promising spatial-temporal learning framework for corn yield estimation under changing meteorological conditions across large spatial regions.},
  author    = {Lin, Tao and Zhong, Renhai and Wang, Yudi and Xu, Jinfan and Jiang, Hao and Xu, Jialu and Ying, Yibin and Rodriguez, Luis and Ting, K. C. and Li, Haifeng},
  doi       = {10.1088/1748-9326/ab66cb},
  issn      = {17489326},
  journal   = {Environmental Research Letters},
  keywords  = {LSTM,attention mechanism,corn,deep learning,multi-task learning,yield estimation},
  number    = {3},
  publisher = {IOP Publishing},
  title     = {{DeepCropNet: a deep spatial-temporal learning framework for county-level corn yield estimation}},
  volume    = {15},
  year      = {2020}
}

@article{Schwalbert2020,
  abstract  = {Soybean yield predictions in Brazil are of great interest for market behavior, to drive governmental policies and to increase global food security. In Brazil soybean yield data generally demand various revisions through the following months after harvest suggesting that there is space for improving the accuracy and the time of yield predictions. This study presents a novel model to perform in-season (“near real-time”) soybean yield forecasts in southern Brazil using Long-Short Term Memory (LSTM), Neural Networks, satellite imagery and weather data. The objectives of this study were to: (i) compare the performance of three different algorithms (multivariate OLS linear regression, random forest and LSTM neural networks) for forecasting soybean yield using NDVI, EVI, land surface temperature and precipitation as independent variables, and (ii) evaluate how early (during the soybean growing season) this method is able to forecast yield with reasonable accuracy. Satellite and weather data were masked using a non-crop-specific layer with field boundaries obtained from the Rural Environment Registry that is mandatory for all farmers in Brazil. Main outcomes from this study were: (i) soybean yield forecasts at municipality-scale with a mean absolute error (MAE) of 0.24 Mg ha−1 at DOY 64 (march 5) (ii) a superior performance of the LSTM neural networks relative to the other algorithms for all the forecast dates except DOY 16 where multivariate OLS linear regression provided the best performance, and (iii) model performance (e.g., MAE) for yield forecast decreased when predictions were performed earlier in the season, with MAE increasing from 0.24 Mg ha−1 to 0.42 Mg ha−1 (last values from OLS regression) when forecast timing changed from DOY 64 (March 5) to DOY 16 (January 6). This research portrays the benefits of integrating statistical techniques, remote sensing, weather to field survey data in order to perform more reliable in-season soybean yield forecasts.},
  author    = {Schwalbert, Ra{\'{i}} A. and Amado, Telmo and Corassa, Geomar and Pott, Luan Pierre and Prasad, P. V.Vara and Ciampitti, Ignacio A.},
  doi       = {10.1016/j.agrformet.2019.107886},
  issn      = {01681923},
  journal   = {Agricultural and Forest Meteorology},
  keywords  = {Deep learning,Long-Short Term Memory,Satellite imagery,Yield forecast},
  number    = {December 2019},
  pages     = {107886},
  publisher = {Elsevier},
  title     = {{Satellite-based soybean yield forecast: Integrating machine learning and weather data for improving crop yield prediction in southern Brazil}},
  volume    = {284},
  year      = {2020}
}

@article{Wang2020,
  abstract = {Timely and accurate forecasting of crop yields is crucial to food security and sustainable development in the agricultural sector. However, winter wheat yield estimation and forecasting on a regional scale still remains challenging. In this study, we established a two-branch deep learning model to predict winter wheat yield in the main producing regions of China at the county level. The first branch of the model was constructed based on the Long Short-Term Memory (LSTM) networks with inputs from meteorological and remote sensing data. Another branch was constructed using Convolution Neural Networks (CNN) to model static soil features. The model was then trained using the detrended statistical yield data during 1982 to 2015 and evaluated by leave-one-year-out-validation. The evaluation results showed a promising performance of the model with the overall R2 and RMSE of 0.77 and 721 kg/ha, respectively. We further conducted yield prediction and uncertainty analysis based on the two-branch model and obtained the forecast accuracy in one month prior to harvest of 0.75 and 732 kg/ha. Results also showed that while yield detrending could potentially introduce higher uncertainty, it had the advantage of improving the model performance in yield prediction.},
  author   = {Wang, Xinlei and Huang, Jianxi and Feng, Quanlong and Yin, Dongqin},
  doi      = {10.3390/rs12111744},
  issn     = {20724292},
  journal  = {Remote Sensing},
  keywords = {Crop yield prediction,Deep learning,Remote sensing,Uncertainty,Winterwheat,Yield detrending},
  number   = {11},
  title    = {{Winter wheat yield prediction at county level and uncertainty analysis in main wheat-producing regions of China with deep learning approaches}},
  volume   = {12},
  year     = {2020}
}

@article{Khaki2020,
  abstract      = {Crop yield prediction is extremely challenging due to its dependence on multiple factors such as crop genotype, environmental factors, management practices, and their interactions. This paper presents a deep learning framework using convolutional neural networks (CNNs) and recurrent neural networks (RNNs) for crop yield prediction based on environmental data and management practices. The proposed CNN-RNN model, along with other popular methods such as random forest (RF), deep fully connected neural networks (DFNN), and LASSO, was used to forecast corn and soybean yield across the entire Corn Belt (including 13 states) in the United States for years 2016, 2017, and 2018 using historical data. The new model achieved a root-mean-square-error (RMSE) 9{\%} and 8{\%} of their respective average yields, substantially outperforming all other methods that were tested. The CNN-RNN has three salient features that make it a potentially useful method for other crop yield prediction studies. (1) The CNN-RNN model was designed to capture the time dependencies of environmental factors and the genetic improvement of seeds over time without having their genotype information. (2) The model demonstrated the capability to generalize the yield prediction to untested environments without significant drop in the prediction accuracy. (3) Coupled with the backpropagation method, the model could reveal the extent to which weather conditions, accuracy of weather predictions, soil conditions, and management practices were able to explain the variation in the crop yields.},
  archiveprefix = {arXiv},
  arxivid       = {1911.09045},
  author        = {Khaki, Saeed and Wang, Lizhi and Archontoulis, Sotirios V.},
  doi           = {10.3389/fpls.2019.01750},
  eprint        = {1911.09045},
  issn          = {1664462X},
  journal       = {Frontiers in Plant Science},
  keywords      = {convolutional neural networks,crop yield prediction,deep learning,feature selection,recurrent neural networks},
  number        = {January},
  pages         = {1--14},
  title         = {{A CNN-RNN Framework for Crop Yield Prediction}},
  volume        = {10},
  year          = {2020}
}

@article{Vincent2008,
  abstract  = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
  address   = {New York, New York, USA},
  author    = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  doi       = {10.1145/1390156.1390294},
  issn      = {1605582050},
  journal   = {Proceedings of the 25th international conference on Machine learning - ICML '08},
  pages     = {1096--1103},
  pmid      = {15540460},
  publisher = {ACM Press},
  title     = {{Extracting and composing robust features with denoising autoencoders}},
  year      = {2008}
}

@article{Russwurm2018b,
  abstract      = {Clouds frequently cover the Earth's surface and pose an omnipresent challenge to optical Earth observation methods. The vast majority of remote sensing approaches either selectively choose single cloud-free observations or employ a pre-classification strategy to identify and mask cloudy pixels. We follow a different strategy and treat cloud coverage as noise that is inherent to the observed satellite data. In prior work, we directly employed a straightforward $\backslash$emph{\{}convolutional long short-term memory{\}} network for vegetation classification without explicit cloud filtering and achieved state-of-the-art classification accuracies. In this work, we investigate this cloud-robustness further by visualizing internal cell activations and performing an ablation experiment on datasets of different cloud coverage. In the visualizations of network states, we identified some cells in which modulation and input gates closed on cloudy pixels. This indicates that the network has internalized a cloud-filtering mechanism without being specifically trained on cloud labels. Overall, our results question the necessity of sophisticated pre-processing pipelines for multi-temporal deep learning approaches.},
  archiveprefix = {arXiv},
  arxivid       = {1811.02471},
  author        = {Ru{\ss}wurm, Marc and K{\"{o}}rner, Marco},
  eprint        = {1811.02471},
  title         = {{Convolutional LSTMs for Cloud-Robust Segmentation of Remote Sensing Imagery}},
  year          = {2018}
}

@article{Cho2014,
  abstract      = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archiveprefix = {arXiv},
  arxivid       = {1406.1078},
  author        = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Sch\-wenk, Holger and Bengio, Yoshua},
  doi           = {10.3115/v1/D14-1179},
  eprint        = {1406.1078},
  issn          = {09205691},
  journal       = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  keywords      = {decoder,for statistical machine translation,rning phrase representations using,rnn encoder},
  pages         = {1724--1734},
  pmid          = {2079951},
  title         = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
  year          = {2014}
}

@inproceedings{Wu2020,
  abstract  = {Rice is one of the most important and valuable crops in the world. People around the world mainly depend on rice as their daily diet. Therefore, efficient rice fields monitoring is a crucial factor in the improvement of rice crop yield estimation, damage evaluation, budget planning, and agricultural resource management. Synthetic aperture radar (SAR) is an effective tool in monitoring agricultural fields because of its ability to provide high resolution images regardless of weather conditions. However, precision agriculture has put higher requirements for SAR data analysis. In recent years, deep learning methods have achieved great success in various remote sensing applications. In this research, two of the most popular deep learning architectures for time series data, namely convolutional long short-term memory (ConvLSTM) and gated recurrent unit (GRU) have been explored and applied to detect rice fields from SAR images in Taiwan. The experimental results showed that time-series deep learning methods for analyzing SAR data have a great potential for improving the rice fields detection.},
  author    = {Wu, Meng Che and Alkhaleefah, Mohammad and Chang, Lena and Chang, Yang Lang and Shie, Ming Hwang and Liu, Shian Jing and Chang, Wen Yen},
  booktitle = {International Geoscience and Remote Sensing Symposium (IGARSS)},
  doi       = {10.1109/IGARSS39084.2020.9324337},
  keywords  = {Deep learning,SAR images,rice fields detection,time series data},
  pages     = {1548--1551},
  publisher = {IEEE},
  title     = {{Recurrent Deep Learning for Rice Fields Detection from SAR Images}},
  year      = {2020}
}

@article{Terliksiz2019,
  abstract = {World population is constantly increasing and it is necessary to have sufficient crop production. Monitoring crop growth and yield estimation are very important for the economic development of a nation. The prediction of crop yield has direct impact on national and international economies and play important role in the food management and food security. Deep learning gains importance on crop monitoring, crop type classification and crop yield estimation applications with the recent advances in image classification using deep Convolutional Neural Networks. Traditional crop yield prediction approaches based on remote sensing consist of classical Machine Learning methods such as Support Vector Machines and Decision Trees. Convolutional Neural Network CNN] and Long-Short Term Memory Network (LSTM] are deep neural network models that are proposed for crop yield prediction recently. This study focused on soybean yield prediction of Lauderdale County, Alabama, USA using 3D CNN model that leverages the spatiotemporal features. The yield is provided from USDA NASS Quick Stat tool for years 2003-2016. The satellite data used is collected from NASA's MODIS land products surface reflectance, land surface temperature and land surface temperature via Google Earth Engine. The root mean squared error (RMSE] is used as the evaluation metric in order to be able to compare the results with other methods that generally uses RMSE as the evaluation metric.},
  author   = {Terliksiz, Anil Suat and Altylar, D. Turgay},
  journal  = {2019 8th International Conference on Agro-Geoinformatics, Agro-Geoinformatics 2019},
  keywords = {Convolutional neural networks,Crop yield prediction,Deep neural networks},
  pages    = {2019--2022},
  title    = {{Use of deep neural networks for crop yield prediction: A case study of soybean yield in lauderdale county, Alabama, USA}},
  year     = {2019}
}


@article{Shidnal2019,
  author    = {Shidnal, Sushila and Latte, Mrityunjaya V. and Kapoor, Ayush},
  doi       = {10.1007/s41870-019-00375-x},
  issn      = {25112112},
  journal   = {International Journal of Information Technology (Singapore)},
  keywords  = {Crop yield,Neural network,Rule-based matrix,Tensor flow},
  pages     = {1--9},
  publisher = {Springer Science+Business Media B.V.},
  title     = {{Crop yield prediction: two-tiered machine learning model approach}},
  year      = {2019}
}

@article{Zhao2020,
  author   = {Zhao, Yan and Potgieter, Andries B. and Zhang, Miao and Wu, Bingfang and Hammer, Graeme L.},
  doi      = {10.3390/rs12061024},
  issn     = {2072-4292},
  journal  = {Remote Sensing},
  keywords = {Atmospheric correction,Crop stress,Google earth engine,Sentinel,Wheat yield prediction},
  number   = {6},
  pages    = {1024},
  title    = {{Predicting Wheat Yield at the Field Scale by Combining High-Resolution Sentinel-2 Satellite Imagery and Crop Modelling}},
  volume   = {12},
  year     = {2020}
}

@online{owidcropyields,
  author  = {Hannah Ritchie and Max Roser},
  note    = {\\\url{https://ourworldindata.org/crop-yields}},
  title   = {Our World in Data: Crop Yields},
  urldate = {2021-06-08},
  year    = {2013}
}